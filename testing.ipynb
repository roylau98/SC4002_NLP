{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_google_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Vector for student: {word2vec_google_news['student']}\", end=\"\\n\")\n",
    "print(f\"Vector for Apple: {word2vec_google_news['Apple']}\", end=\"\\n\")\n",
    "print(f\"Vector for apple: {word2vec_google_news['apple']}\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"word2vec_google_news data type: {type(word2vec_google_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_words = word2vec_google_news.index_to_key\n",
    "print(f\"Size of Vocab: {len(all_words)}\")\n",
    "print(f\"Vocabulary of first 50:\\n{all_words[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the Punkt tokenizer models (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the content of the 'eng.testa' file\n",
    "with open('CoNLL2003_dataset/eng.testa', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Tokenize the content into sentences\n",
    "sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "# Find the maximum sentence length in terms of tokens\n",
    "max_length = max(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "print(f\"The maximum sentence length in terms of tokens is: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x = torch.tensor(\n",
    "    [[-1.1251, -1.7150, -2.0017, -2.1880, -2.0354, -2.1409],\n",
    "    [-0.9344, -1.9967, -1.9087, -2.3024, -2.1959, -2.1907],\n",
    "    [-0.4184, -2.2436, -2.3116, -3.2885, -2.7615, -3.3192],\n",
    "    [-0.1347, -3.0930, -3.2008, -4.5437, -4.0678, -4.4064],\n",
    "    [-0.1766, -2.5108, -3.5405, -4.3358, -3.7204, -4.2453],\n",
    "    [-0.1142, -3.0301, -3.7901, -4.9302, -3.8639, -4.7302],\n",
    "    [-0.1754, -2.4690, -3.7175, -4.5138, -3.4773, -4.5978],\n",
    "    [-0.5185, -1.7804, -2.4450, -3.4879, -2.5408, -3.2208],\n",
    "    [-0.0786, -3.4615, -4.1117, -5.6984, -3.9626, -5.2123]]\n",
    ")\n",
    "x = torch.tensor(\n",
    "    [[-1, -2, -3, -4, -6, -7]],\n",
    ")\n",
    "print(f\"x \\n {x}\")\n",
    "x0 = torch.abs(x)\n",
    "# x0 = F.log_softmax(x, dim=1)\n",
    "print(f\"x0 \\n {x0}\")\n",
    "predicted_labels = torch.argmax(x0, dim=1)\n",
    "print(f\"predicted_labels \\n {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "df = pd.read_csv(\"TREC_dataset/train.csv\", sep=\",\") \n",
    "\n",
    "# Assuming your text column is named 'text_column', create a new column with word count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Filter rows with exactly 8 words\n",
    "filtered_df = df[df['word_count'] == 8]\n",
    "\n",
    "# Drop the word_count column if you don't need it anymore\n",
    "filtered_df = filtered_df.drop(columns=['word_count'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('eugene_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "df = pd.read_csv(\"TREC_dataset/test.csv\", sep=\",\") \n",
    "\n",
    "# Assuming your text column is named 'text_column', create a new column with word count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Filter rows with exactly 8 words\n",
    "filtered_df = df[df['word_count'] == 8]\n",
    "\n",
    "# Drop the word_count column if you don't need it anymore\n",
    "filtered_df = filtered_df.drop(columns=['word_count'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('eugene_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the punkt tokenizer if you haven't already\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"TREC_dataset/train.csv\", sep=\",\") \n",
    "\n",
    "# Assuming your text column is named 'text_column', tokenize the text and create a new column with word count\n",
    "df['tokenized_text'] = df['text_column'].apply(lambda x: word_tokenize(str(x)))\n",
    "df['word_count'] = df['tokenized_text'].apply(len)\n",
    "\n",
    "# Filter rows with exactly 8 words\n",
    "filtered_df = df[df['word_count'] == 8]\n",
    "\n",
    "# Drop the extra columns if you don't need them anymore\n",
    "filtered_df = filtered_df.drop(columns=['tokenized_text', 'word_count'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('eugene_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the punkt tokenizer if you haven't already\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"TREC_dataset/test.csv\", sep=\",\") \n",
    "\n",
    "# Assuming your text column is named 'text_column', tokenize the text and create a new column with word count\n",
    "df['tokenized_text'] = df['text_column'].apply(lambda x: word_tokenize(str(x)))\n",
    "df['word_count'] = df['tokenized_text'].apply(len)\n",
    "\n",
    "# Filter rows with exactly 8 words\n",
    "filtered_df = df[df['word_count'] == 8]\n",
    "\n",
    "# Drop the extra columns if you don't need them anymore\n",
    "filtered_df = filtered_df.drop(columns=['tokenized_text', 'word_count'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('eugene_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
