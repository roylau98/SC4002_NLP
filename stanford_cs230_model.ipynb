{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.923216Z",
     "start_time": "2023-10-16T14:37:13.814857Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.990985Z",
     "start_time": "2023-10-16T14:37:13.822618Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thousands': 0,\n",
       " 'of': 1,\n",
       " 'demonstrators': 2,\n",
       " 'have': 3,\n",
       " 'marched': 4,\n",
       " 'through': 5,\n",
       " 'London': 6,\n",
       " 'to': 7,\n",
       " 'protest': 8,\n",
       " 'the': 9,\n",
       " 'war': 10,\n",
       " 'in': 11,\n",
       " 'Iraq': 12,\n",
       " 'and': 13,\n",
       " 'demand': 14,\n",
       " 'withdrawal': 15,\n",
       " 'British': 16,\n",
       " 'troops': 17,\n",
       " 'from': 18,\n",
       " 'that': 19,\n",
       " 'country': 20,\n",
       " '.': 21,\n",
       " 'Families': 22,\n",
       " 'soldiers': 23,\n",
       " 'killed': 24,\n",
       " 'conflict': 25,\n",
       " 'joined': 26,\n",
       " 'protesters': 27,\n",
       " 'who': 28,\n",
       " 'carried': 29,\n",
       " 'banners': 30,\n",
       " 'with': 31,\n",
       " 'such': 32,\n",
       " 'slogans': 33,\n",
       " 'as': 34,\n",
       " '\"': 35,\n",
       " 'Bush': 36,\n",
       " 'Number': 37,\n",
       " 'One': 38,\n",
       " 'Terrorist': 39,\n",
       " 'Stop': 40,\n",
       " 'Bombings': 41,\n",
       " 'They': 42,\n",
       " 'Houses': 43,\n",
       " 'Parliament': 44,\n",
       " 'a': 45,\n",
       " 'rally': 46,\n",
       " 'Hyde': 47,\n",
       " 'Park': 48,\n",
       " 'Police': 49,\n",
       " 'put': 50,\n",
       " 'number': 51,\n",
       " 'marchers': 52,\n",
       " 'at': 53,\n",
       " '10,000': 54,\n",
       " 'while': 55,\n",
       " 'organizers': 56,\n",
       " 'claimed': 57,\n",
       " 'it': 58,\n",
       " 'was': 59,\n",
       " '1,00,000': 60,\n",
       " 'The': 61,\n",
       " 'comes': 62,\n",
       " 'on': 63,\n",
       " 'eve': 64,\n",
       " 'annual': 65,\n",
       " 'conference': 66,\n",
       " 'Britain': 67,\n",
       " \"'s\": 68,\n",
       " 'ruling': 69,\n",
       " 'Labor': 70,\n",
       " 'Party': 71,\n",
       " 'southern': 72,\n",
       " 'English': 73,\n",
       " 'seaside': 74,\n",
       " 'resort': 75,\n",
       " 'Brighton': 76,\n",
       " 'party': 77,\n",
       " 'is': 78,\n",
       " 'divided': 79,\n",
       " 'over': 80,\n",
       " 'participation': 81,\n",
       " 'continued': 82,\n",
       " 'deployment': 83,\n",
       " '8,500': 84,\n",
       " 'march': 85,\n",
       " 'came': 86,\n",
       " 'ahead': 87,\n",
       " 'anti-war': 88,\n",
       " 'protests': 89,\n",
       " 'today': 90,\n",
       " 'other': 91,\n",
       " 'cities': 92,\n",
       " ',': 93,\n",
       " 'including': 94,\n",
       " 'Rome': 95,\n",
       " 'Paris': 96,\n",
       " 'Madrid': 97,\n",
       " 'International': 98,\n",
       " 'Atomic': 99,\n",
       " 'Energy': 100,\n",
       " 'Agency': 101,\n",
       " 'hold': 102,\n",
       " 'second': 103,\n",
       " 'day': 104,\n",
       " 'talks': 105,\n",
       " 'Vienna': 106,\n",
       " 'Wednesday': 107,\n",
       " 'how': 108,\n",
       " 'respond': 109,\n",
       " 'Iran': 110,\n",
       " 'resumption': 111,\n",
       " 'low-level': 112,\n",
       " 'uranium': 113,\n",
       " 'conversion': 114,\n",
       " 'this': 115,\n",
       " 'week': 116,\n",
       " 'restarted': 117,\n",
       " 'parts': 118,\n",
       " 'process': 119,\n",
       " 'its': 120,\n",
       " 'Isfahan': 121,\n",
       " 'nuclear': 122,\n",
       " 'plant': 123,\n",
       " 'Iranian': 124,\n",
       " 'officials': 125,\n",
       " 'say': 126,\n",
       " 'they': 127,\n",
       " 'expect': 128,\n",
       " 'get': 129,\n",
       " 'access': 130,\n",
       " 'sealed': 131,\n",
       " 'sensitive': 132,\n",
       " 'after': 133,\n",
       " 'an': 134,\n",
       " 'IAEA': 135,\n",
       " 'surveillance': 136,\n",
       " 'system': 137,\n",
       " 'begins': 138,\n",
       " 'functioning': 139,\n",
       " 'Mr.': 140,\n",
       " 'Nour': 141,\n",
       " 'arrested': 142,\n",
       " 'January': 143,\n",
       " 'spent': 144,\n",
       " 'six': 145,\n",
       " 'weeks': 146,\n",
       " 'Cairo': 147,\n",
       " 'jail': 148,\n",
       " 'before': 149,\n",
       " 'his': 150,\n",
       " 'release': 151,\n",
       " 'bond': 152,\n",
       " 'last': 153,\n",
       " 'In': 154,\n",
       " 'letter': 155,\n",
       " 'Egyptian': 156,\n",
       " 'President': 157,\n",
       " 'Hosni': 158,\n",
       " 'Mubarak': 159,\n",
       " 'New': 160,\n",
       " 'York-based': 161,\n",
       " 'Human': 162,\n",
       " 'Rights': 163,\n",
       " 'Watch': 164,\n",
       " 'said': 165,\n",
       " 'dismayed': 166,\n",
       " 'by': 167,\n",
       " 'what': 168,\n",
       " 'called': 169,\n",
       " 'radical': 170,\n",
       " 'intolerance': 171,\n",
       " 'toward': 172,\n",
       " 'political': 173,\n",
       " 'dissent': 174,\n",
       " 'U.S.': 175,\n",
       " 'State': 176,\n",
       " 'Department': 177,\n",
       " 'European': 178,\n",
       " 'parliament': 179,\n",
       " 'also': 180,\n",
       " 'voiced': 181,\n",
       " 'concern': 182,\n",
       " 'Pakistani': 183,\n",
       " 'military': 184,\n",
       " '14': 185,\n",
       " 'about': 186,\n",
       " '40': 187,\n",
       " 'went': 188,\n",
       " 'missing': 189,\n",
       " 'following': 190,\n",
       " 'attack': 191,\n",
       " 'security': 192,\n",
       " 'checkpoint': 193,\n",
       " 'been': 194,\n",
       " 'found': 195,\n",
       " 'neighboring': 196,\n",
       " 'Afghanistan': 197,\n",
       " 'Officials': 198,\n",
       " 'Frontier': 199,\n",
       " 'Corps': 200,\n",
       " 'paramilitary': 201,\n",
       " 'disappeared': 202,\n",
       " 'Mohmand': 203,\n",
       " 'tribal': 204,\n",
       " 'region': 205,\n",
       " 'Taliban': 206,\n",
       " 'insurgent': 207,\n",
       " 'along': 208,\n",
       " 'Afghan': 209,\n",
       " 'border': 210,\n",
       " 'earlier': 211,\n",
       " 'Military': 212,\n",
       " 'spokesman': 213,\n",
       " 'Major': 214,\n",
       " 'General': 215,\n",
       " 'Athar': 216,\n",
       " 'Abbas': 217,\n",
       " 'told': 218,\n",
       " 'reporters': 219,\n",
       " 'Thursday': 220,\n",
       " 'authorities': 221,\n",
       " 'handed': 222,\n",
       " 'consulate': 223,\n",
       " 'Jalalabad': 224,\n",
       " 'were': 225,\n",
       " 'being': 226,\n",
       " 'flown': 227,\n",
       " 'back': 228,\n",
       " 'Pakistan': 229,\n",
       " 'militants': 230,\n",
       " 'captured': 231,\n",
       " '10': 232,\n",
       " 'during': 233,\n",
       " 'post': 234,\n",
       " 'but': 235,\n",
       " 'could': 236,\n",
       " 'not': 237,\n",
       " 'verify': 238,\n",
       " 'claim': 239,\n",
       " 'On': 240,\n",
       " 'least': 241,\n",
       " '36': 242,\n",
       " 'fighting': 243,\n",
       " 'Bajaur': 244,\n",
       " 'has': 245,\n",
       " 'twice': 246,\n",
       " 'declared': 247,\n",
       " 'victory': 248,\n",
       " 'there': 249,\n",
       " 'offensives': 250,\n",
       " 'aimed': 251,\n",
       " 'clearing': 252,\n",
       " 'area': 253,\n",
       " 'insurgents': 254,\n",
       " 'linked': 255,\n",
       " 'al-Qaida': 256,\n",
       " 'Thailand': 257,\n",
       " 'named': 258,\n",
       " 'committee': 259,\n",
       " 'begin': 260,\n",
       " 'writing': 261,\n",
       " 'new': 262,\n",
       " 'constitution': 263,\n",
       " 'coup': 264,\n",
       " 'month': 265,\n",
       " 'At': 266,\n",
       " 'Group': 267,\n",
       " 'Eight': 268,\n",
       " 'summit': 269,\n",
       " 'Scotland': 270,\n",
       " 'Japanese': 271,\n",
       " 'Prime': 272,\n",
       " 'Minister': 273,\n",
       " 'Junichiro': 274,\n",
       " 'Koizumi': 275,\n",
       " 'he': 276,\n",
       " 'outraged': 277,\n",
       " 'attacks': 278,\n",
       " 'He': 279,\n",
       " 'noted': 280,\n",
       " 'terrorist': 281,\n",
       " 'acts': 282,\n",
       " 'must': 283,\n",
       " 'be': 284,\n",
       " 'forgivable': 285,\n",
       " 'Sarin': 286,\n",
       " 'gas': 287,\n",
       " 'Tokyo': 288,\n",
       " 'subway': 289,\n",
       " '1995': 290,\n",
       " '12': 291,\n",
       " 'people': 292,\n",
       " 'injured': 293,\n",
       " 'thousands': 294,\n",
       " 'A': 295,\n",
       " 'human': 296,\n",
       " 'rights': 297,\n",
       " 'group': 298,\n",
       " 'Asian': 299,\n",
       " 'leaders': 300,\n",
       " 'increase': 301,\n",
       " 'pressure': 302,\n",
       " 'Burma': 303,\n",
       " 'hasten': 304,\n",
       " 'democratic': 305,\n",
       " 'reforms': 306,\n",
       " 'stop': 307,\n",
       " 'abuses': 308,\n",
       " 'Alternative': 309,\n",
       " 'ASEAN': 310,\n",
       " 'Network': 311,\n",
       " 'for': 312,\n",
       " 'Association': 313,\n",
       " 'Southeast': 314,\n",
       " 'Nations': 315,\n",
       " 'meeting': 316,\n",
       " 'should': 317,\n",
       " 'consider': 318,\n",
       " 'options': 319,\n",
       " 'dealing': 320,\n",
       " 'It': 321,\n",
       " 'supporting': 322,\n",
       " 'possible': 323,\n",
       " 'resolution': 324,\n",
       " 'United': 325,\n",
       " 'Security': 326,\n",
       " 'Council': 327,\n",
       " 'urged': 328,\n",
       " 'acknowledge': 329,\n",
       " 'many': 330,\n",
       " 'problems': 331,\n",
       " 'caused': 332,\n",
       " 'regime': 333,\n",
       " 'accuses': 334,\n",
       " 'government': 335,\n",
       " 'involvement': 336,\n",
       " 'illegal': 337,\n",
       " 'drug': 338,\n",
       " 'trafficking': 339,\n",
       " 'especially': 340,\n",
       " 'against': 341,\n",
       " 'some': 342,\n",
       " 'ethnic': 343,\n",
       " 'groups': 344,\n",
       " 'Iraqi': 345,\n",
       " 'gunmen': 346,\n",
       " 'member': 347,\n",
       " 'secular': 348,\n",
       " 'coalition': 349,\n",
       " 'led': 350,\n",
       " 'former': 351,\n",
       " 'prime': 352,\n",
       " 'minister': 353,\n",
       " 'Ayad': 354,\n",
       " 'Allawi': 355,\n",
       " 'Faras': 356,\n",
       " 'al-Jabouri': 357,\n",
       " 'shot': 358,\n",
       " 'Saturday': 359,\n",
       " 'raided': 360,\n",
       " 'home': 361,\n",
       " 'near': 362,\n",
       " 'northern': 363,\n",
       " 'city': 364,\n",
       " 'Mosul': 365,\n",
       " '<pad>': 366,\n",
       " 'UNK': 367}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_path = \"stanford_cs230_data/words.txt\"\n",
    "vocab = {}\n",
    "with open(words_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        vocab[l] = i\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d8959b6d5bbdcb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.991286Z",
     "start_time": "2023-10-16T14:37:13.838609Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'I-geo': 1, 'I-gpe': 2, 'I-per': 3, 'I-org': 4, 'I-tim': 5}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_path = \"stanford_cs230_data/tags.txt\"\n",
    "tag_map = {}\n",
    "with open(tags_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        tag_map[l] = i\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5cb4a69d3a6f4b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.991538Z",
     "start_time": "2023-10-16T14:37:13.847742Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentences: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21], [22, 1, 23, 24, 11, 9, 25, 26, 9, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 35, 13, 35, 40, 9, 41, 21, 35], [42, 4, 18, 9, 43, 1, 44, 7, 45, 46, 11, 47, 48, 21], [49, 50, 9, 51, 1, 52, 53, 54, 55, 56, 57, 58, 59, 60, 21], [61, 8, 62, 63, 9, 64, 1, 9, 65, 66, 1, 67, 68, 69, 70, 71, 11, 9, 72, 73, 74, 75, 1, 76, 21], [61, 77, 78, 79, 80, 67, 68, 81, 11, 9, 12, 25, 13, 9, 82, 83, 1, 84, 16, 17, 11, 19, 20, 21], [61, 6, 85, 86, 87, 1, 88, 89, 90, 11, 91, 92, 93, 94, 95, 93, 96, 93, 13, 97, 21], [61, 98, 99, 100, 101, 78, 7, 102, 103, 104, 1, 105, 11, 106, 107, 63, 108, 7, 109, 7, 110, 68, 111, 1, 112, 113, 114, 21], [110, 115, 116, 117, 118, 1, 9, 114, 119, 53, 120, 121, 122, 123, 21], [124, 125, 126, 127, 128, 7, 129, 130, 7, 131, 132, 118, 1, 9, 123, 107, 93, 133, 134, 135, 136, 137, 138, 139, 21]]\n",
      "train_labels: [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], [0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_file = \"stanford_cs230_data/train/sentences.txt\"\n",
    "train_labels_file = \"stanford_cs230_data/train/labels.txt\"\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "with open(train_sentences_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        train_sentences.append(s)\n",
    "\n",
    "with open(train_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        train_labels.append(l)\n",
    "        \n",
    "print(f\"train_sentences: {train_sentences}\")\n",
    "print(f\"train_labels: {train_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "736b1f18b861288f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.991974Z",
     "start_time": "2023-10-16T14:37:13.869511Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_sentences: [[140, 141, 59, 142, 11, 143, 13, 144, 145, 146, 11, 45, 147, 148, 93, 149, 150, 151, 63, 152, 153, 116, 21], [154, 45, 155, 7, 156, 157, 158, 159, 93, 9, 160, 161, 162, 163, 164, 165, 58, 59, 166, 167, 168, 58, 169, 147, 68, 35, 170, 171, 35, 172, 173, 174, 21], [61, 175, 176, 177, 13, 9, 178, 179, 180, 181, 182, 21], [183, 184, 125, 126, 185, 1, 186, 187, 183, 23, 28, 188, 189, 190, 134, 191, 63, 45, 192, 193, 3, 194, 195, 11, 196, 197, 21], [198, 126, 9, 199, 200, 201, 17, 202, 18, 9, 203, 204, 205, 133, 45, 206, 207, 191, 208, 9, 209, 210, 211, 115, 116, 21], [212, 213, 214, 215, 216, 217, 218, 219, 220, 19, 209, 221, 222, 80, 9, 17, 7, 9, 183, 223, 11, 224, 13, 9, 23, 225, 226, 227, 228, 7, 229, 21], [206, 230, 165, 127, 231, 232, 23, 233, 9, 191, 63, 9, 201, 234, 93, 235, 125, 236, 237, 238, 9, 239, 21], [240, 107, 93, 183, 125, 165, 232, 201, 23, 13, 53, 241, 242, 230, 225, 24, 11, 243, 11, 9, 20, 68, 244, 204, 205, 21], [61, 183, 184, 245, 246, 247, 248, 249, 190, 250, 251, 53, 252, 9, 253, 1, 254, 255, 7, 9, 206, 13, 256, 21], [257, 68, 184, 245, 258, 45, 259, 7, 260, 9, 119, 1, 261, 45, 262, 263, 93, 190, 45, 184, 264, 153, 265, 21]]\n",
      "val_labels: [[3, 3, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 3, 3, 3, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 3, 3, 3, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "val_sentences_file = \"stanford_cs230_data/val/sentences.txt\"\n",
    "val_labels_file = \"stanford_cs230_data/val/labels.txt\"\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "with open(val_sentences_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        val_sentences.append(s)\n",
    "\n",
    "with open(val_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        val_labels.append(l)\n",
    "\n",
    "print(f\"val_sentences: {val_sentences}\")\n",
    "print(f\"val_labels: {val_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.011887Z",
     "start_time": "2023-10-16T14:37:13.882822Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        # if params.cuda:\n",
    "        #     batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.012307Z",
     "start_time": "2023-10-16T14:37:13.894126Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
    "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
    "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
    "    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n",
    "\n",
    "    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n",
    "    you can go about defining your own network.\n",
    "\n",
    "    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        \"\"\"\n",
    "        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n",
    "        required are:\n",
    "\n",
    "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "\n",
    "        Args:\n",
    "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        # for more details on how to use it, check out the documentation\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This function defines how we use the components of our network to operate on an input batch.\n",
    "\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "        s, _ = self.lstm(s)\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "        s = s.view(-1, s.shape[2])\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.012907Z",
     "start_time": "2023-10-16T14:37:13.899592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.013393Z",
     "start_time": "2023-10-16T14:37:13.913339Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014581Z",
     "start_time": "2023-10-16T14:37:13.927378Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % 10 == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a1feee96f562ef39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014871Z",
     "start_time": "2023-10-16T14:37:13.939867Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch = next(data_iterator)\n",
    "\n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "798027bbaa11f757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014985Z",
     "start_time": "2023-10-16T14:37:13.951561Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        metrics\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(\n",
    "            train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator,\n",
    "              metrics, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(\n",
    "            val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        val_metrics = evaluate(\n",
    "            model, loss_fn, val_data_iterator, metrics, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a981f205b1133b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.015072Z",
     "start_time": "2023-10-16T14:37:13.960431Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.520501Z",
     "start_time": "2023-10-16T14:37:13.975258Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 250.08it/s, loss=1.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.025 ; loss: 1.860\n",
      "- Eval metrics : accuracy: 0.695 ; loss: 1.586\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.32it/s, loss=1.380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.849 ; loss: 1.481\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 1.143\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 399.88it/s, loss=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.896\n",
      "- Eval metrics : accuracy: 0.822 ; loss: 0.763\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.33it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.581\n",
      "- Eval metrics : accuracy: 0.822 ; loss: 0.798\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.26it/s, loss=0.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.591\n",
      "- Eval metrics : accuracy: 0.822 ; loss: 0.800\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.36it/s, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.539\n",
      "- Eval metrics : accuracy: 0.822 ; loss: 0.774\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.36it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.462\n",
      "- Eval metrics : accuracy: 0.822 ; loss: 0.769\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 333.29it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.857 ; loss: 0.411\n",
      "- Eval metrics : accuracy: 0.818 ; loss: 0.787\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 399.95it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.874 ; loss: 0.385\n",
      "- Eval metrics : accuracy: 0.810 ; loss: 0.787\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 400.09it/s, loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.891 ; loss: 0.340\n",
      "- Eval metrics : accuracy: 0.814 ; loss: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(368, 50, 50, 6)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_and_evaluate(model, train_sentences, train_labels, val_sentences, val_labels, 10, 5, optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dbc5724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentences: [[266, 9, 267, 1, 268, 269, 11, 270, 93, 271, 272, 273, 274, 275, 165, 276, 78, 277, 167, 9, 6, 278, 21], [279, 280, 281, 282, 283, 237, 284, 285, 21], [286, 287, 278, 63, 9, 288, 289, 137, 11, 290, 24, 291, 292, 13, 293, 294, 21], [295, 296, 297, 298, 245, 169, 63, 299, 300, 7, 301, 302, 63, 303, 7, 304, 305, 306, 13, 307, 296, 297, 308, 21], [61, 309, 310, 311, 312, 303, 165, 125, 18, 9, 313, 1, 314, 299, 315, 316, 115, 116, 317, 318, 262, 319, 11, 320, 31, 303, 21], [321, 165, 300, 317, 318, 322, 45, 323, 324, 63, 303, 167, 9, 325, 315, 326, 327, 21], [61, 298, 180, 328, 310, 300, 7, 329, 9, 330, 192, 331, 332, 167, 303, 68, 184, 333, 21], [61, 297, 298, 334, 303, 68, 335, 1, 336, 11, 337, 338, 339, 13, 296, 297, 308, 93, 340, 341, 342, 343, 344, 11, 303, 21], [345, 125, 126, 346, 3, 24, 45, 347, 1, 9, 348, 349, 350, 167, 351, 345, 352, 353, 354, 355, 21], [198, 126, 356, 357, 59, 358, 359, 133, 346, 360, 150, 361, 362, 9, 363, 364, 1, 365, 21]]\n",
      "test_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences_file = \"stanford_cs230_data/test/sentences.txt\"\n",
    "test_labels_file = \"stanford_cs230_data/test/labels.txt\"\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "with open(test_sentences_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        test_sentences.append(s)\n",
    "\n",
    "with open(test_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        test_labels.append(l)\n",
    "\n",
    "print(f\"test_sentences: {test_sentences}\")\n",
    "print(f\"test_labels: {test_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9be8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([[-1.1183, -1.8462, -1.9015, -2.2424, -1.7853, -2.3856],\n",
      "        [-0.0347, -4.0720, -4.6996, -6.4464, -5.3254, -6.4676],\n",
      "        [-0.0620, -3.1846, -4.5866, -6.2881, -5.3532, -6.2509],\n",
      "        [-0.0664, -3.1607, -4.5125, -6.1850, -5.0840, -5.9278],\n",
      "        [-0.1254, -2.8633, -3.4542, -5.1714, -4.1124, -4.9431],\n",
      "        [-0.3470, -1.8092, -2.9626, -4.4258, -3.2586, -3.5999],\n",
      "        [-0.0321, -3.9309, -4.9840, -7.1018, -5.8339, -6.5657],\n",
      "        [-0.0643, -3.3626, -4.2797, -6.2738, -4.7983, -5.6078],\n",
      "        [-0.0248, -4.0293, -5.5215, -7.9063, -6.3390, -7.4565],\n",
      "        [-0.0183, -4.4299, -5.4817, -8.1877, -6.6653, -7.6226],\n",
      "        [-0.1449, -2.3843, -3.7928, -5.6601, -4.6798, -4.8985],\n",
      "        [-0.1270, -2.8042, -3.2322, -5.5873, -4.5766, -5.2498],\n",
      "        [-0.4357, -1.6742, -2.7314, -4.0512, -3.0342, -3.3497],\n",
      "        [-0.0762, -3.2254, -3.9211, -5.7970, -4.7833, -6.0405],\n",
      "        [-0.0490, -3.7194, -4.3494, -6.2713, -4.9353, -6.4255],\n",
      "        [-0.2149, -2.4598, -2.8075, -4.7056, -3.5202, -4.7171],\n",
      "        [-0.1918, -2.4597, -3.0769, -4.5537, -3.7654, -4.6835],\n",
      "        [-0.2520, -2.2355, -3.1323, -4.3333, -3.2072, -3.9847],\n",
      "        [-0.2710, -2.2526, -2.9439, -4.2513, -3.2613, -3.6122],\n",
      "        [-0.8876, -1.1925, -2.2942, -3.2620, -2.5903, -2.6490],\n",
      "        [-0.0525, -3.4905, -4.5173, -6.2739, -5.2636, -5.9090]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([21, 6])\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))\n",
    "test_data_iterator = data_iterator(test_sentences, test_labels, len(test_sentences), 1, shuffle=True)\n",
    "test_batch, labels_batch = next(test_data_iterator)\n",
    "model_output = model(test_batch)\n",
    "print(model_output)\n",
    "print(model_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ff42671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output \n",
      " tensor([[-1.1183, -1.8462, -1.9015, -2.2424, -1.7853, -2.3856],\n",
      "        [-0.0347, -4.0720, -4.6996, -6.4464, -5.3254, -6.4676],\n",
      "        [-0.0620, -3.1846, -4.5866, -6.2881, -5.3532, -6.2509],\n",
      "        [-0.0664, -3.1607, -4.5125, -6.1850, -5.0840, -5.9278],\n",
      "        [-0.1254, -2.8633, -3.4542, -5.1714, -4.1124, -4.9431],\n",
      "        [-0.3470, -1.8092, -2.9626, -4.4258, -3.2586, -3.5999],\n",
      "        [-0.0321, -3.9309, -4.9840, -7.1018, -5.8339, -6.5657],\n",
      "        [-0.0643, -3.3626, -4.2797, -6.2738, -4.7983, -5.6078],\n",
      "        [-0.0248, -4.0293, -5.5215, -7.9063, -6.3390, -7.4565],\n",
      "        [-0.0183, -4.4299, -5.4817, -8.1877, -6.6653, -7.6226],\n",
      "        [-0.1449, -2.3843, -3.7928, -5.6601, -4.6798, -4.8985],\n",
      "        [-0.1270, -2.8042, -3.2322, -5.5873, -4.5766, -5.2498],\n",
      "        [-0.4357, -1.6742, -2.7314, -4.0512, -3.0342, -3.3497],\n",
      "        [-0.0762, -3.2254, -3.9211, -5.7970, -4.7833, -6.0405],\n",
      "        [-0.0490, -3.7194, -4.3494, -6.2713, -4.9353, -6.4255],\n",
      "        [-0.2149, -2.4598, -2.8075, -4.7056, -3.5202, -4.7171],\n",
      "        [-0.1918, -2.4597, -3.0769, -4.5537, -3.7654, -4.6835],\n",
      "        [-0.2520, -2.2355, -3.1323, -4.3333, -3.2072, -3.9847],\n",
      "        [-0.2710, -2.2526, -2.9439, -4.2513, -3.2613, -3.6122],\n",
      "        [-0.8876, -1.1925, -2.2942, -3.2620, -2.5903, -2.6490],\n",
      "        [-0.0525, -3.4905, -4.5173, -6.2739, -5.2636, -5.9090]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "predicted_labels \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_output \\n {model_output}\")\n",
    "# predicted_labels = torch.argmax(model_output, dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# predicted_labels = torch.argmax(torch.abs(model_output), dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "predicted_labels = np.argmax(model_output.detach().numpy(), axis=1)\n",
    "print(f\"predicted_labels \\n {predicted_labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
