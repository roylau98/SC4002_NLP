{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "(a) Specify all the 5 classes you used after converting from the original label set to the new setting.\n",
    "\n",
    "(b) Describe what aggregation methods you have tried and which is finally adopted (and why). Explain the detailed function of the aggregation method you used. If you have tested different aggregation methods, list their accuracy results to support your claim.\n",
    "\n",
    "(c) Describe what neural network you used to produce the final vector representation of each word and what are the mathematical functions used for the forward computation (i.e., from the pretrained word vectors to the final label of each word). Give the detailed setting of the network including which parameters are being updated, what are their sizes, and what is the length of the final vector representation of each word to be fed to the softmax classifier.\n",
    "\n",
    "(d) Report how many epochs you used for training, as well as the running time.\n",
    "\n",
    "(e) Report the accuracy on the test set, as well as the accuracy on the development set for each\n",
    "epoch during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:14:42.714649400Z",
     "start_time": "2023-11-09T06:14:33.801448800Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4d9acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:42.476010400Z",
     "start_time": "2023-11-09T06:14:42.721648400Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446beddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:42.516318200Z",
     "start_time": "2023-11-09T06:15:42.498977300Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the code below tokenizes the modified csv file containing TREC dataset and proceeds to convert the tokens (words) into word2vec indexs. In addition, we format the labels to the correct input dimensions for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47079fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:42.528147500Z",
     "start_time": "2023-11-09T06:15:42.519315800Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    setences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "    return setences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read the dataset from the csv files. Then we split the train dataset for training and validation respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e894b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.453351800Z",
     "start_time": "2023-11-09T06:15:42.541150800Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5 # no. of labels after merging is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de5ed02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.520655100Z",
     "start_time": "2023-11-09T06:15:43.456319300Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.533309Z",
     "start_time": "2023-11-09T06:15:43.527627Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        \n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        \n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network used below is an LSTM-based network that predicts the sentence classification tags for each sentence. It consists of three main components:\n",
    "\n",
    "1. An embedding layer that maps each token to its embedding vector.\n",
    "2. An LSTM layer that processes the embedded tokens and produces LSTM outputs for each token.\n",
    "3. An aggregation layer that summarizes each token output vector into one vector.\n",
    "4. Dropout layer for regularization purposes.\n",
    "5. Batch normalization layers for regularization purposes.\n",
    "6. Two fully connected layer (fc). The forward computation involves the following steps:\n",
    "\n",
    "Embedding                 : Mapping tokens to their embedding vectors using the embedding layer.  \n",
    "LSTM                      : Applying the LSTM on the embedded tokens, resulting in LSTM outputs for each token.  \n",
    "Reshaping                 : Making the output contiguous in memory and reshaping it for further processing.  \n",
    "Mean pooling              : Summarizes each token output into one single word embedding.  \n",
    "Fully Connected Layers    : Applying the fully connected layers to obtain word embeddings before the softmax.  \n",
    "Log Softmax               : Applying log softmax to the output for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.550857200Z",
     "start_time": "2023-11-09T06:15:43.533309Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights,freeze=True, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # initialize the weights using xavier uniform initialization\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "        \n",
    "        # batch normalization layer\n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.005) \n",
    "        \n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 150)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(150)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(150, number_of_tags)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len.\n",
    "            lengths: (list) contains the original lengths of the sequences in the batch.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size, seq_len, embedding_dim = padded_output.size()\n",
    "        s = self.batch_norm1(padded_output.view(-1, embedding_dim))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        s = s.view(batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        s = torch.sum(s, dim=1)  # sum pooling\n",
    "        s = self.fc1(s)\n",
    "        s = self.batch_norm2(s)\n",
    "        s = self.relu(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc2(s)\n",
    "        out = self.batch_norm3(s)\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.562520600Z",
     "start_time": "2023-11-09T06:15:43.559994500Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.571890400Z",
     "start_time": "2023-11-09T06:15:43.569891600Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Eval code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:15:43.598572500Z",
     "start_time": "2023-11-09T06:15:43.582888100Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    train_loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        train_loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(train_loss_avg()))\n",
    "    return train_loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    validation_loss_avg = RunningAverage()\n",
    "    validation_accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        validation_loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        validation_accuracy_avg.update(accuracy_val)\n",
    "\n",
    "    print(f\"{validation_loss_avg()=}\")\n",
    "    print(f\"{validation_accuracy_avg()=}\")\n",
    "    \n",
    "    return validation_loss_avg(), validation_accuracy_avg()\n",
    "    \n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    accuracies_across_epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        loss, accuracy = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "        accuracies_across_epochs.append(accuracy)\n",
    "\n",
    "        if early_stopper.early_stop(loss):             \n",
    "            break\n",
    "    \n",
    "    return accuracies_across_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:16:20.815276200Z",
     "start_time": "2023-11-09T06:15:43.594574300Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 76.89it/s, loss=1.023] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.6746328671773275\n",
      "validation_accuracy_avg()=0.7958333333333333\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 211.54it/s, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5340773443380992\n",
      "validation_accuracy_avg()=0.8229166666666666\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 205.10it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5625243484973907\n",
      "validation_accuracy_avg()=0.8020833333333334\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 199.26it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5533554335435231\n",
      "validation_accuracy_avg()=0.8145833333333333\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 198.28it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4748883287111918\n",
      "validation_accuracy_avg()=0.8479166666666667\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 196.07it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.47094121277332307\n",
      "validation_accuracy_avg()=0.84375\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 203.64it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.535432513554891\n",
      "validation_accuracy_avg()=0.8375\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 206.15it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5202665368715922\n",
      "validation_accuracy_avg()=0.84375\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 210.18it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5248562912146251\n",
      "validation_accuracy_avg()=0.85\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 204.06it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5952154080073039\n",
      "validation_accuracy_avg()=0.8354166666666667\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 198.00it/s, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5507069950302442\n",
      "validation_accuracy_avg()=0.85625\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 210.76it/s, loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5727186034123103\n",
      "validation_accuracy_avg()=0.8541666666666666\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 220.78it/s, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.5869713122646014\n",
      "validation_accuracy_avg()=0.85625\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 213.81it/s, loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.6254219273726146\n",
      "validation_accuracy_avg()=0.8354166666666667\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 219.32it/s, loss=0.076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.6272685259580613\n",
      "validation_accuracy_avg()=0.85625\n",
      "execution_time=13.565598249435425\n"
     ]
    }
   ],
   "source": [
    "model = Net(embedding_weights, 300, 5, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "if (os.path.isfile(\"model_weights_sum_pooling.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights_sum_pooling.pth'))\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    accuracies_across_epochs = train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 100, 32, optimizer, loss_fn)\n",
    "    execution_time = time.time() - start_time\n",
    "    torch.save(model.state_dict(), 'model_weights_sum_pooling.pth')\n",
    "    \n",
    "print(f\"{execution_time=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 torch.Size([20, 300])\n",
      "lstm.weight_hh_l0 torch.Size([20, 5])\n",
      "lstm.bias_ih_l0 torch.Size([20])\n",
      "lstm.bias_hh_l0 torch.Size([20])\n",
      "batch_norm1.weight torch.Size([5])\n",
      "batch_norm1.bias torch.Size([5])\n",
      "fc1.weight torch.Size([150, 5])\n",
      "fc1.bias torch.Size([150])\n",
      "batch_norm2.weight torch.Size([150])\n",
      "batch_norm2.bias torch.Size([150])\n",
      "fc2.weight torch.Size([5, 150])\n",
      "fc2.bias torch.Size([5])\n",
      "batch_norm3.weight torch.Size([5])\n",
      "batch_norm3.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec728a-fcae-4748-9a93-0f3ccbf76c74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be8268f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T06:16:20.857341500Z",
     "start_time": "2023-11-09T06:16:20.822276700Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.866\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c8082e4307f46d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Development Set for Epoch 1: 0.7958\n",
      "Accuracy on Development Set for Epoch 2: 0.8229\n",
      "Accuracy on Development Set for Epoch 3: 0.8021\n",
      "Accuracy on Development Set for Epoch 4: 0.8146\n",
      "Accuracy on Development Set for Epoch 5: 0.8479\n",
      "Accuracy on Development Set for Epoch 6: 0.8438\n",
      "Accuracy on Development Set for Epoch 7: 0.8375\n",
      "Accuracy on Development Set for Epoch 8: 0.8438\n",
      "Accuracy on Development Set for Epoch 9: 0.8500\n",
      "Accuracy on Development Set for Epoch 10: 0.8354\n",
      "Accuracy on Development Set for Epoch 11: 0.8562\n",
      "Accuracy on Development Set for Epoch 12: 0.8542\n",
      "Accuracy on Development Set for Epoch 13: 0.8562\n",
      "Accuracy on Development Set for Epoch 14: 0.8354\n",
      "Accuracy on Development Set for Epoch 15: 0.8562\n"
     ]
    }
   ],
   "source": [
    "# display accuracies on development set per epoch\n",
    "for epoch, accuracy in enumerate(accuracies_across_epochs):\n",
    "    print(f\"Accuracy on Development Set for Epoch {epoch + 1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T06:16:45.448983200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = What is a squirrel ?, label = 0\n",
      "sentence = Is Singapore located in Southeast Asia ?, label = 3\n",
      "sentence = Is Singapore in China ?, label = 1\n",
      "sentence = Name 11 famous martyrs ., label = 4\n",
      "sentence = What ISPs exist in the Caribbean ?, label = 4\n",
      "sentence = How many cars are manufactured every day ?, label = 4\n"
     ]
    }
   ],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel ?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia ?\")\n",
    "print_sentence_label(\"Is Singapore in China ?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
