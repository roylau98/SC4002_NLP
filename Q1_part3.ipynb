{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a1e2800bec565",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Question 1.3\n",
    "(a) Discuss how you deal with new words in the training set which are not found in the pretrained\n",
    "dictionary. Likewise, how do you deal with new words in the test set which are not found in\n",
    "either the pretrained dictionary or the training set? Show the corresponding code snippet.\n",
    "\n",
    "(b) Describe what neural network you used to produce the final vector representation of each\n",
    "word and what are the mathematical functions used for the forward computation (i.e., from\n",
    "the pretrained word vectors to the final label of each word). Give the detailed setting of the\n",
    "network including which parameters are being updated, what are their sizes, and what is the\n",
    "length of the final vector representation of each word to be fed to the softmax classifier.\n",
    "\n",
    "(c) Report how many epochs you used for training, as well as the running time.\n",
    "\n",
    "(d) Report the f1 score on the test set, as well as the f1 score on the development set for each\n",
    "epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792c2587301a7713",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from seqeval.metrics import f1_score as f1_score_seqeval\n",
    "from seqeval.metrics import classification_report\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793ed4b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901dd99a9dc19fc",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the code below reads and parses a CoNLL-formatted file, extracting sentences and their corresponding NER tags for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6a19218c802f5a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    tag = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:  # Empty line indicates the end of a sentence\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                if word == '-DOCSTART-':\n",
    "                    continue\n",
    "                ner_tag = parts[-1]\n",
    "                sentence.append(word)\n",
    "                tag.append(ner_tag)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag)\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32fdf6c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data, train_tags = read_conll_file('CoNLL2003_dataset/eng.train')\n",
    "val_data, val_tags = read_conll_file('CoNLL2003_dataset/eng.testa')\n",
    "test_data, test_tags = read_conll_file('CoNLL2003_dataset/eng.testb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7024e1fa37c85",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code below extract unique tags given a list of tags extracted from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edd41267bbf65df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_unique_tags(allTags):\n",
    "    uniqueTags = []\n",
    "    flatten_sentences = list(chain(*allTags))\n",
    "    for i in flatten_sentences:\n",
    "        if not i in uniqueTags:\n",
    "            uniqueTags.append(i)\n",
    "    return uniqueTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8474de4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_tags = get_unique_tags(train_tags+val_tags+test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e14aa0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-ORG': 0,\n",
       " 'O': 1,\n",
       " 'I-MISC': 2,\n",
       " 'I-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-LOC': 5,\n",
       " 'B-MISC': 6,\n",
       " 'B-ORG': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map = {}\n",
    "for index, tag in enumerate(unique_tags):\n",
    "    tag_map[tag] = index\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8af74310bc5c56",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "'Indexify' function takes data and tags as input and converts them into indexed sentences and labels using vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896551380fc0f3cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def indexify(data, tag):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        sentences.append(s)\n",
    "\n",
    "    for sentence in tag:\n",
    "        l = [tag_map[label] for label in sentence]\n",
    "        labels.append(l)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "train_sentences, train_labels = indexify(train_data, train_tags)\n",
    "val_sentences, val_labels = indexify(val_data, val_tags)\n",
    "test_sentences, test_labels = indexify(test_data, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f5939b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False, cuda=True):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of the longest sentence in the batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        if cuda:\n",
    "            batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545ceeb9f22c7db",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The neural network used below is an LSTM-based network that predicts named entity recognition (NER) tags for each\n",
    "token in a sentence. It consists of three main components:\n",
    "\n",
    "1. An embedding layer that maps each token to its embedding vector.\n",
    "2. An LSTM layer that processes the embedded tokens and produces LSTM outputs for each token.\n",
    "3. A fully connected layer (fc) that converts the LSTM output for each token to a distribution over NER tags.\n",
    "The forward computation involves the following steps:\n",
    "\n",
    "Embedding: Mapping tokens to their embedding vectors using the embedding layer.\n",
    "LSTM: Applying the LSTM on the embedded tokens, resulting in LSTM outputs for each token.\n",
    "Reshaping: Making the output contiguous in memory and reshaping it for further processing.\n",
    "Fully Connected Layer: Applying the fully connected layer to obtain the output for each token before the softmax.\n",
    "Log Softmax: Applying log softmax to the output for numerical stability.\n",
    "\n",
    "The final vector representation of each word is obtained from the LSTM output, and it is of size lstm_hidden_dim. This representation is then used to compute the distribution over NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab03416b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        \"\"\"\n",
    "        We define a recurrent network that predicts the NER tags for each token in the sentence. The components\n",
    "        required are:\n",
    "\n",
    "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "\n",
    "        Args:\n",
    "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        # for more details on how to use it, check out the documentation\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This function defines how we use the components of our network to operate on an input batch.\n",
    "\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "        s, _ = self.lstm(s)\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "        s = s.view(-1, s.shape[2])\n",
    "        s = self.batch_norm1(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecfde36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADDING tokens.\n",
    "\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9378985",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c42a94f6-a47b-433e-a76b-c4b2b1b05a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743d52fbe278ffd",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training and Eval code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1257b3e51121267c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % 10 == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "                \n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Training f1 score: \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5108a7c6bee7ac2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Validation f1 score : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc3fd180abcdac4",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        metrics\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    \n",
    "    model=model.to(device)\n",
    "    train_metrics_list = []\n",
    "    val_metrics_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(\n",
    "            train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True, cuda=torch.cuda.is_available())\n",
    "        train_metrics = train(model, optimizer, loss_fn, train_data_iterator,\n",
    "              metrics, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(\n",
    "            val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False, cuda=torch.cuda.is_available())\n",
    "        val_metrics = evaluate(\n",
    "            model, loss_fn, val_data_iterator, metrics, num_steps)\n",
    "\n",
    "        train_metrics_list.append(train_metrics)\n",
    "        val_metrics_list.append(val_metrics)\n",
    "\n",
    "        if early_stopper.early_stop(val_metrics[\"loss\"]):             \n",
    "            break\n",
    "\n",
    "    return train_metrics_list, val_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15a16fc310ca129c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "inv_tag_map = {v: k for k, v in tag_map.items()}\n",
    "\n",
    "def id_to_words(sentence):\n",
    "    new_sentence = [inv_vocab[i] for i in sentence]\n",
    "    return new_sentence\n",
    "\n",
    "def id_to_labels(labels):\n",
    "    new_sentence = [inv_tag_map[i] for i in labels]\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6d35020ba4d82",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Various function to determine the accuracy/stats of the model:\n",
    "\n",
    "- accuracy(outputs, labels): Computes the accuracy of the model's predictions given the outputs and labels, excluding\n",
    "padding terms.\n",
    "- calculate_multiclass_f1_score(outputs, labels): Calculates the multiclass F1 score for the model's predictions, excluding padding terms. It converts class IDs to labels and returns the F1 score.\n",
    "- classification_report_gen(outputs, labels): Generates a classification report for the model's predictions, excluding padding terms. It converts class IDs to labels and returns the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1c277eb4f0a2df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_multiclass_f1_score(outputs, labels):\n",
    "    \n",
    "    labels = labels.ravel()\n",
    "    mask = (labels >= 0)  \n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    outputs = outputs[mask]\n",
    "    labels = labels[mask]\n",
    "    outputs = id_to_labels(outputs)\n",
    "    labels = id_to_labels(labels)\n",
    "    outputs = np.expand_dims(outputs, axis=0)\n",
    "    labels = np.expand_dims(labels, axis=0)\n",
    "    outputs = outputs.tolist()\n",
    "    labels = labels.tolist()\n",
    "    f1= f1_score_seqeval(labels, outputs)\n",
    "    return f1\n",
    "\n",
    "def classification_report_gen(outputs, labels):\n",
    "    labels = labels.ravel()\n",
    "    mask = (labels >= 0)  \n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    outputs = outputs[mask]\n",
    "    labels = labels[mask]\n",
    "    outputs = id_to_labels(outputs)\n",
    "    labels = id_to_labels(labels)\n",
    "    outputs = np.expand_dims(outputs, axis=0)\n",
    "    labels = np.expand_dims(labels, axis=0)\n",
    "    outputs = outputs.tolist()\n",
    "    labels = labels.tolist()\n",
    "    # return classification_report(labels, outputs, mode='strict', scheme=IOB1)\n",
    "    return classification_report(labels, outputs)\n",
    "\n",
    "metrics = {\n",
    "    'f1_seqeval': calculate_multiclass_f1_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dff023bab2ccc8c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available, and if so, use it\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c16af97b386e1",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba1711517d90c788",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tag_map): 8\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 181.41it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.734 ; loss: 0.159\n",
      "- Validation f1 score : f1_seqeval: 0.759 ; loss: 0.120\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 188.39it/s, loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.818 ; loss: 0.097\n",
      "- Validation f1 score : f1_seqeval: 0.773 ; loss: 0.106\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 186.59it/s, loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.858 ; loss: 0.071\n",
      "- Validation f1 score : f1_seqeval: 0.778 ; loss: 0.108\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 191.47it/s, loss=0.054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.892 ; loss: 0.051\n",
      "- Validation f1 score : f1_seqeval: 0.779 ; loss: 0.113\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 191.78it/s, loss=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.928 ; loss: 0.037\n",
      "- Validation f1 score : f1_seqeval: 0.777 ; loss: 0.127\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:16<00:00, 171.31it/s, loss=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.945 ; loss: 0.026\n",
      "- Validation f1 score : f1_seqeval: 0.768 ; loss: 0.148\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:16<00:00, 174.28it/s, loss=0.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.953 ; loss: 0.022\n",
      "- Validation f1 score : f1_seqeval: 0.787 ; loss: 0.161\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 183.87it/s, loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.964 ; loss: 0.018\n",
      "- Validation f1 score : f1_seqeval: 0.776 ; loss: 0.166\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 179.01it/s, loss=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.969 ; loss: 0.015\n",
      "- Validation f1 score : f1_seqeval: 0.783 ; loss: 0.174\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 184.44it/s, loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.975 ; loss: 0.012\n",
      "- Validation f1 score : f1_seqeval: 0.789 ; loss: 0.178\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 189.20it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.974 ; loss: 0.011\n",
      "- Validation f1 score : f1_seqeval: 0.789 ; loss: 0.183\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 188.38it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.978 ; loss: 0.011\n",
      "- Validation f1 score : f1_seqeval: 0.789 ; loss: 0.185\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 188.67it/s, loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.974 ; loss: 0.010\n",
      "- Validation f1 score : f1_seqeval: 0.795 ; loss: 0.195\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 186.35it/s, loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.980 ; loss: 0.009\n",
      "- Validation f1 score : f1_seqeval: 0.781 ; loss: 0.204\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 186.07it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.979 ; loss: 0.009\n",
      "- Validation f1 score : f1_seqeval: 0.785 ; loss: 0.209\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 190.30it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.987 ; loss: 0.007\n",
      "- Validation f1 score : f1_seqeval: 0.790 ; loss: 0.204\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 199.01it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.985 ; loss: 0.009\n",
      "- Validation f1 score : f1_seqeval: 0.789 ; loss: 0.215\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:15<00:00, 184.89it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.984 ; loss: 0.008\n",
      "- Validation f1 score : f1_seqeval: 0.796 ; loss: 0.217\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 193.32it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.988 ; loss: 0.006\n",
      "- Validation f1 score : f1_seqeval: 0.793 ; loss: 0.213\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:14<00:00, 193.23it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training f1 score: f1_seqeval: 0.986 ; loss: 0.006\n",
      "- Validation f1 score : f1_seqeval: 0.794 ; loss: 0.222\n",
      "total training time taken: 330.02195024490356\n"
     ]
    }
   ],
   "source": [
    "# manually change vocab size (unique no. of words) and change label size (unique no. of labels) for now\n",
    "model = Net(embedding_weights, 300, 300, len(tag_map))\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Q1.3b\n",
    "print(\"len(tag_map):\", len(tag_map))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "if os.path.isfile(\"model_weights_q1_part3.pth\"):\n",
    "    model.load_state_dict(torch.load('model_weights_q1_part3.pth'))\n",
    "    \n",
    "else:\n",
    "    start_time = time.time()\n",
    "    train_metrics_list, val_metrics_list = train_and_evaluate(model, train_sentences, train_labels, val_sentences,\n",
    "                                                            val_labels, 100, 5, optimizer, loss_fn, metrics)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(\"total training time taken:\", total_time)\n",
    "    torch.save(model.state_dict(), 'model_weights_q1_part3.pth')\n",
    "    with open(\"training_information/Q1_part3/train_metrics_list.pkl\", 'wb') as file:\n",
    "        pickle.dump(train_metrics_list, file)\n",
    "\n",
    "    # Save the dictionary to a file\n",
    "    with open(\"training_information/Q1_part3/val_metrics_list.pkl\", 'wb') as file:\n",
    "        pickle.dump(val_metrics_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63745660c449d95a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Coallate the data and plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e989c21e62a8c907",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Process metrics for plot\n",
    "train_f1_list = []\n",
    "val_f1_list = []\n",
    "for i in train_metrics_list:\n",
    "    train_f1_list.append(i[\"f1_seqeval\"])\n",
    "for i in val_metrics_list:\n",
    "    val_f1_list.append(i[\"f1_seqeval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cffe9b7e7e2c27d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training F1: 0.7342, Validation F1: 0.7585\n",
      "Epoch 2 - Training F1: 0.8185, Validation F1: 0.7734\n",
      "Epoch 3 - Training F1: 0.8578, Validation F1: 0.7780\n",
      "Epoch 4 - Training F1: 0.8918, Validation F1: 0.7786\n",
      "Epoch 5 - Training F1: 0.9280, Validation F1: 0.7769\n",
      "Epoch 6 - Training F1: 0.9453, Validation F1: 0.7682\n",
      "Epoch 7 - Training F1: 0.9534, Validation F1: 0.7873\n",
      "Epoch 8 - Training F1: 0.9644, Validation F1: 0.7762\n",
      "Epoch 9 - Training F1: 0.9692, Validation F1: 0.7826\n",
      "Epoch 10 - Training F1: 0.9750, Validation F1: 0.7892\n",
      "Epoch 11 - Training F1: 0.9742, Validation F1: 0.7891\n",
      "Epoch 12 - Training F1: 0.9783, Validation F1: 0.7887\n",
      "Epoch 13 - Training F1: 0.9742, Validation F1: 0.7951\n",
      "Epoch 14 - Training F1: 0.9796, Validation F1: 0.7809\n",
      "Epoch 15 - Training F1: 0.9790, Validation F1: 0.7851\n",
      "Epoch 16 - Training F1: 0.9872, Validation F1: 0.7899\n",
      "Epoch 17 - Training F1: 0.9849, Validation F1: 0.7892\n",
      "Epoch 18 - Training F1: 0.9836, Validation F1: 0.7964\n",
      "Epoch 19 - Training F1: 0.9880, Validation F1: 0.7931\n",
      "Epoch 20 - Training F1: 0.9858, Validation F1: 0.7937\n",
      "Best Training F1 Score: 0.9880 (Epoch 19)\n",
      "Best Validation F1 Score: 0.7964 (Epoch 18)\n"
     ]
    }
   ],
   "source": [
    "best_train_f1 = 0\n",
    "best_val_f1 = 0\n",
    "best_train_epoch = 0\n",
    "best_val_epoch = 0\n",
    "\n",
    "for epoch, (train_f1, val_f1) in enumerate(zip(train_f1_list, val_f1_list), start=1):\n",
    "    print(f\"Epoch {epoch} - Training F1: {train_f1:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Check for the best F1 score on both training and validation sets\n",
    "    if train_f1 > best_train_f1:\n",
    "        best_train_f1 = train_f1\n",
    "        best_train_epoch = epoch\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_epoch = epoch\n",
    "\n",
    "# Print the best F1 scores and corresponding epochs\n",
    "print(f\"Best Training F1 Score: {best_train_f1:.4f} (Epoch {best_train_epoch})\")\n",
    "print(f\"Best Validation F1 Score: {best_val_f1:.4f} (Epoch {best_val_epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cff3dfcc62afe41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1,len(train_metrics_list)+1),train_f1_list, label='Training f1-score')\n",
    "plt.plot(range(1,len(val_metrics_list)+1),val_f1_list,label='Test f1-score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a828ca18d1dc36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Get test data and labels through data iterator\n",
    "'''Test batch- tensor of n_sentences x max_len_sentence\n",
    "   Labels_batch- tensor of n_sentences x max_len_sentence\n",
    "   Test sentences- list of n_sentences x sentence_length(no padding)'''\n",
    "test_data_iterator = data_iterator(test_sentences, test_labels, len(test_sentences), len(test_sentences),\n",
    "                                   shuffle=True, cuda=torch.cuda.is_available())\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "511985f95fadbf35",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_output = model(test_batch)\n",
    "model_output_numpy = model_output.cpu().detach().numpy()\n",
    "labels_batch_numpy = labels_batch.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5b8219f7bfefe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \n",
      " 3453\n",
      "Number of predicted word labels \n",
      " 428172\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.argmax(model_output.cpu().detach().numpy(), axis=1)\n",
    "print(f\"Number of sentences: \\n {len(test_sentences)}\")\n",
    "print(f\"Number of predicted word labels \\n {len(predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdbe5300158625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample question: ['Yevgeny', 'Kafelnikov', 'UNK', 'Russia', 'UNK', 'beat', 'Jim', 'Courier', 'UNK', 'U.S.', 'UNK', 'UNK', 'UNK', 'UNK']\n",
      "sample_label_predict: ['I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'O']\n",
      "sample_label_true: ['I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "#Show model output on sample from test set\n",
    "sample_output = model(test_batch[10].unsqueeze(0)).cpu()\n",
    "sample_question = id_to_words(test_sentences[10])\n",
    "sample_mask = (labels_batch[10] >= 0).cpu()\n",
    "sample_label_predict = np.argmax(sample_output.detach().numpy(), axis=1)[sample_mask]\n",
    "sample_label_true = labels_batch[10][sample_mask]\n",
    "print(f\"sample question: {sample_question}\")\n",
    "print(f\"sample_label_predict: {id_to_labels(sample_label_predict)}\")\n",
    "print(f\"sample_label_true: {id_to_labels(sample_label_true.cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aa6aeabc92700cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set f1 score: 0.7611279826464208\n"
     ]
    }
   ],
   "source": [
    "#compute f1 score for test set\n",
    "f1_score_seqeval = calculate_multiclass_f1_score(model_output_numpy, labels_batch_numpy)\n",
    "print(f\"Test set f1 score: {f1_score_seqeval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86138013e1a9730f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_class_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.81      0.83      0.82      1661\n",
      "        MISC       0.62      0.65      0.63       702\n",
      "         ORG       0.65      0.73      0.69      1655\n",
      "         PER       0.85      0.83      0.84      1611\n",
      "\n",
      "   micro avg       0.74      0.78      0.76      5629\n",
      "   macro avg       0.73      0.76      0.74      5629\n",
      "weighted avg       0.75      0.78      0.76      5629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_class_report = classification_report_gen(model_output_numpy, labels_batch_numpy)\n",
    "print(f\"model_class_report: \\n{model_class_report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
