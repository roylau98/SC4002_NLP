{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:42:21.005577600Z",
     "start_time": "2023-11-08T05:42:12.616155700Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4d9acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:39.467735400Z",
     "start_time": "2023-11-08T05:42:21.009602500Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446beddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:39.544517900Z",
     "start_time": "2023-11-08T05:43:39.504701100Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47079fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:40.929073100Z",
     "start_time": "2023-11-08T05:43:40.916859100Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    setences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "    return setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e894b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:41.896105400Z",
     "start_time": "2023-11-08T05:43:40.958408700Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de5ed02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.348796600Z",
     "start_time": "2023-11-08T05:43:41.914110600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.361369200Z",
     "start_time": "2023-11-08T05:43:42.350797200Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.378599100Z",
     "start_time": "2023-11-08T05:43:42.372890900Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.005) \n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 150)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(150)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(150, number_of_tags)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len.\n",
    "            lengths: (list) contains the original lengths of the sequences in the batch.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size, seq_len, embedding_dim = padded_output.size()\n",
    "        s = self.batch_norm1(padded_output.view(-1, embedding_dim))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        s = s.view(batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        s = torch.mean(s, dim=1)  # mean pooling\n",
    "        s = self.fc1(s)\n",
    "        s = self.batch_norm2(s)\n",
    "        s = self.relu(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc2(s)\n",
    "        out = self.batch_norm3(s)\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.401289200Z",
     "start_time": "2023-11-08T05:43:42.382564800Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.402289300Z",
     "start_time": "2023-11-08T05:43:42.387292400Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:43:42.418914Z",
     "start_time": "2023-11-08T05:43:42.412287500Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "    return loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    loss_avg = RunningAverage()\n",
    "    accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        accuracy_avg.update(accuracy_val)\n",
    "\n",
    "    print(f\"{loss_avg()=}\")\n",
    "    print(f\"{accuracy_avg()=}\")\n",
    "    \n",
    "    return loss_avg()\n",
    "\n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        a = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "\n",
    "        if early_stopper.early_stop(a):             \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:47:43.409287600Z",
     "start_time": "2023-11-08T05:43:42.422914900Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 239.57it/s, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5931776801745097\n",
      "accuracy_avg()=0.8333333333333334\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 226.57it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4438645283381144\n",
      "accuracy_avg()=0.8583333333333333\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 232.42it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3652214139699936\n",
      "accuracy_avg()=0.86875\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 226.47it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.33203662584225335\n",
      "accuracy_avg()=0.8895833333333333\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 221.17it/s, loss=0.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3748878916104635\n",
      "accuracy_avg()=0.8854166666666666\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 230.54it/s, loss=0.075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.32437676986058556\n",
      "accuracy_avg()=0.8895833333333333\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 233.67it/s, loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3926639430224895\n",
      "accuracy_avg()=0.8916666666666667\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 230.54it/s, loss=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3559552599986394\n",
      "accuracy_avg()=0.8916666666666667\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 221.26it/s, loss=0.027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.36616131439805033\n",
      "accuracy_avg()=0.9020833333333333\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 223.73it/s, loss=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3923511698842049\n",
      "accuracy_avg()=0.8979166666666667\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 228.15it/s, loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.3912060388674339\n",
      "accuracy_avg()=0.9083333333333333\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 232.02it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4120727633436521\n",
      "accuracy_avg()=0.9083333333333333\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 231.40it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4047132093459368\n",
      "accuracy_avg()=0.91875\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 229.78it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.41088332620759804\n",
      "accuracy_avg()=0.9125\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 220.65it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.41694176917274794\n",
      "accuracy_avg()=0.9145833333333333\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 223.51it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.39453705586493015\n",
      "accuracy_avg()=0.9125\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 224.49it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4253175818671783\n",
      "accuracy_avg()=0.9125\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 226.00it/s, loss=0.063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5740124901135762\n",
      "accuracy_avg()=0.8729166666666667\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 224.16it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4409350569049517\n",
      "accuracy_avg()=0.9\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 223.19it/s, loss=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4216279455150167\n",
      "accuracy_avg()=0.9125\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 226.75it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.39616493427505095\n",
      "accuracy_avg()=0.91875\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 222.22it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.41535095274448397\n",
      "accuracy_avg()=0.9125\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 223.51it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4093786273151636\n",
      "accuracy_avg()=0.9166666666666666\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 223.49it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.41064072170605265\n",
      "accuracy_avg()=0.9166666666666666\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 224.82it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.41120635320742926\n",
      "accuracy_avg()=0.9145833333333333\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 221.43it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4164795440311233\n",
      "accuracy_avg()=0.9145833333333333\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 226.41it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4193449675726394\n",
      "accuracy_avg()=0.9145833333333333\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 224.96it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4253995473186175\n",
      "accuracy_avg()=0.9145833333333333\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:00<00:00, 222.03it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.43138997874533136\n",
      "accuracy_avg()=0.9145833333333333\n"
     ]
    }
   ],
   "source": [
    "model = Net(embedding_weights, 300, 300, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# if (os.path.isfile(\"model_weights_average_pooling.pth\")):\n",
    "#     model.load_state_dict(torch.load('model_weights_average_pooling.pth'))\n",
    "# else:\n",
    "#     train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 100, 32, optimizer, loss_fn)\n",
    "#     torch.save(model.state_dict(), 'model_weights_average_pooling.pth')\n",
    "train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 100, 32, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec728a-fcae-4748-9a93-0f3ccbf76c74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9be8268f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:47:43.485376100Z",
     "start_time": "2023-11-08T05:47:43.414296800Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.914\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T05:47:43.485376100Z",
     "start_time": "2023-11-08T05:47:43.456611100Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = What is a squirrel?, label = 0\n",
      "sentence = Is Singapore located in Southeast Asia?, label = 3\n",
      "sentence = Is Singapore in China?, label = 1\n",
      "sentence = Name 11 famous martyrs ., label = 4\n",
      "sentence = What ISPs exist in the Caribbean ?, label = 4\n",
      "sentence = How many cars are manufactured every day?, label = 4\n"
     ]
    }
   ],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia?\")\n",
    "print_sentence_label(\"Is Singapore in China?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
