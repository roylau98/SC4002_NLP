{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16fcb8704f34334f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:10:09.782156Z",
     "start_time": "2023-11-09T14:10:09.392532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wongyipun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e4d9acf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:11:50.085510Z",
     "start_time": "2023-11-09T14:10:09.582895Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "446beddd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:11:50.111556Z",
     "start_time": "2023-11-09T14:11:50.099575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47079fad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:39.460844Z",
     "start_time": "2023-11-09T14:16:39.393262Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    setences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "    return setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e894b3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.789718Z",
     "start_time": "2023-11-09T14:16:39.426597Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3de5ed02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.940316Z",
     "start_time": "2023-11-09T14:16:40.793968Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.960904Z",
     "start_time": "2023-11-09T14:16:40.949478Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.978288Z",
     "start_time": "2023-11-09T14:16:40.965603Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.005) \n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 150)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(150)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(150, number_of_tags)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len.\n",
    "            lengths: (list) contains the original lengths of the sequences in the batch.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size, seq_len, embedding_dim = padded_output.size()\n",
    "        s = self.batch_norm1(padded_output.view(-1, embedding_dim))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        s = s.view(batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        s = torch.mean(s, dim=1)  # mean pooling\n",
    "        s = self.fc1(s)\n",
    "        s = self.batch_norm2(s)\n",
    "        s = self.relu(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc2(s)\n",
    "        out = self.batch_norm3(s)\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:35:26.007275Z",
     "start_time": "2023-11-09T14:35:25.993396Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e07330c216283eda",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.992519Z",
     "start_time": "2023-11-09T14:16:40.986802Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84b8f1d154a50053",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:41.026544Z",
     "start_time": "2023-11-09T14:16:40.997010Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    train_loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        train_loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(train_loss_avg()))\n",
    "    return train_loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    validation_loss_avg = RunningAverage()\n",
    "    validation_accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        validation_loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        validation_accuracy_avg.update(accuracy_val)\n",
    "\n",
    "    print(f\"{validation_loss_avg()=}\")\n",
    "    print(f\"{validation_accuracy_avg()=}\")\n",
    "    \n",
    "    return validation_loss_avg(), validation_accuracy_avg()\n",
    "\n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "\n",
    "    accuracies_across_epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        loss, accuracy = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "        accuracies_across_epochs.append(accuracy)\n",
    "\n",
    "        if early_stopper.early_stop(loss):             \n",
    "            break\n",
    "    \n",
    "    return accuracies_across_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:32:57.061327Z",
     "start_time": "2023-11-09T14:16:41.017775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:16<00:00,  9.18it/s, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.6000736494859059\n",
      "validation_accuracy_avg()=0.7916666666666666\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:35<00:00,  4.33it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4150438328584035\n",
      "validation_accuracy_avg()=0.86875\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:21<00:00,  7.11it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3972196807463964\n",
      "validation_accuracy_avg()=0.8729166666666667\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:38<00:00,  3.97it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4355363786220551\n",
      "validation_accuracy_avg()=0.8520833333333333\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:23<00:00,  6.50it/s, loss=0.140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3759088784456253\n",
      "validation_accuracy_avg()=0.88125\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:27<00:00,  5.55it/s, loss=0.084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.397687799235185\n",
      "validation_accuracy_avg()=0.9\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:19<00:00,  7.97it/s, loss=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.37606920761366686\n",
      "validation_accuracy_avg()=0.8979166666666667\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.51it/s, loss=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3487920694674055\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:26<00:00,  5.72it/s, loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3289921686053276\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:21<00:00,  7.05it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3334142063433925\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.37it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3327262558663885\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:23<00:00,  6.68it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.326776633101205\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:36<00:00,  4.22it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.32776074931025506\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.47it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3358973562096556\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.54it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.33955281917005775\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.58it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3485751474276185\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.41it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.35303945541381837\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:19<00:00,  7.70it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3574858196079731\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:31<00:00,  4.89it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3612914279103279\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.47it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3646058419098457\n",
      "validation_accuracy_avg()=0.9166666666666666\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:19<00:00,  8.03it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3688585352152586\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:24<00:00,  6.35it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3727054024115205\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.44it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3776312740519643\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:22<00:00,  6.74it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.38030828442424536\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:21<00:00,  7.28it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3856260158121586\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:22<00:00,  6.85it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3882795878996452\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.58it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.39392043743282557\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:22<00:00,  6.87it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.39517919241140287\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:29<00:00,  5.15it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3997637750580907\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:25<00:00,  6.11it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.40213455505048235\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:25<00:00,  5.93it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.40653019019713005\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:16<00:00,  9.37it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.40869243660320836\n",
      "validation_accuracy_avg()=0.9145833333333333\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:14<00:00, 10.28it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.41384379553298156\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:15<00:00, 10.25it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.41861017666136224\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:15<00:00,  9.79it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4220994886321326\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:14<00:00, 10.68it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4270956201168398\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:13<00:00, 11.04it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.42947502148648103\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:20<00:00,  7.37it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4326964066363871\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:18<00:00,  8.22it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.44079083039735756\n",
      "validation_accuracy_avg()=0.9125\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:13<00:00, 11.31it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.43894548863172533\n",
      "validation_accuracy_avg()=0.9125\n",
      "execution_time=915.479480266571\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "model = Net(embedding_weights, 300, 300, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "if (os.path.isfile(\"model_weights_average_pooling.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights_average_pooling.pth'))\n",
    "else:\n",
    "    start_time = time()\n",
    "    accuracies_across_epochs = train_and_evaluate(model, X_train_tokenized_indexified, y_train_formatted, X_val_tokenized_indexified, y_val_formatted, 100, 32, optimizer, loss_fn)\n",
    "    execution_time = time() - start_time\n",
    "    torch.save(model.state_dict(), 'model_weights_average_pooling.pth')\n",
    "    \n",
    "print(f\"{execution_time=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9be8268f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:35:32.735076Z",
     "start_time": "2023-11-09T14:35:31.898900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.93\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Development Set for Epoch 1: 0.7916666666666666\n",
      "Accuracy on Development Set for Epoch 2: 0.86875\n",
      "Accuracy on Development Set for Epoch 3: 0.8729166666666667\n",
      "Accuracy on Development Set for Epoch 4: 0.8520833333333333\n",
      "Accuracy on Development Set for Epoch 5: 0.88125\n",
      "Accuracy on Development Set for Epoch 6: 0.9\n",
      "Accuracy on Development Set for Epoch 7: 0.8979166666666667\n",
      "Accuracy on Development Set for Epoch 8: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 9: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 10: 0.9125\n",
      "Accuracy on Development Set for Epoch 11: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 12: 0.9125\n",
      "Accuracy on Development Set for Epoch 13: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 14: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 15: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 16: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 17: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 18: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 19: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 20: 0.9166666666666666\n",
      "Accuracy on Development Set for Epoch 21: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 22: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 23: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 24: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 25: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 26: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 27: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 28: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 29: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 30: 0.9125\n",
      "Accuracy on Development Set for Epoch 31: 0.9125\n",
      "Accuracy on Development Set for Epoch 32: 0.9145833333333333\n",
      "Accuracy on Development Set for Epoch 33: 0.9125\n",
      "Accuracy on Development Set for Epoch 34: 0.9125\n",
      "Accuracy on Development Set for Epoch 35: 0.9125\n",
      "Accuracy on Development Set for Epoch 36: 0.9125\n",
      "Accuracy on Development Set for Epoch 37: 0.9125\n",
      "Accuracy on Development Set for Epoch 38: 0.9125\n",
      "Accuracy on Development Set for Epoch 39: 0.9125\n",
      "Accuracy on Development Set for Epoch 40: 0.9125\n"
     ]
    }
   ],
   "source": [
    "# display accuracies on development set per epoch\n",
    "for epoch, accuracy in enumerate(accuracies_across_epochs):\n",
    "    print(f\"Accuracy on Development Set for Epoch {epoch + 1}: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-09T14:32:57.036402Z"
    }
   },
   "id": "5d221991c1181e13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Test Accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dec728a-fcae-4748-9a93-0f3ccbf76c74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T14:32:57.767799Z",
     "start_time": "2023-11-09T14:32:57.760799Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia?\")\n",
    "print_sentence_label(\"Is Singapore in China?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
