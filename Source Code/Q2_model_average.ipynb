{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c756e21cf7002415",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Question 2\n",
    "(a) Specify all the 5 classes you used after converting from the original label set to the new setting.\n",
    "\n",
    "(b) Describe what aggregation methods you have tried and which is finally adopted (and why). Explain the detailed function of the aggregation method you used. If you have tested different aggregation methods, list their accuracy results to support your claim.\n",
    "\n",
    "(c) Describe what neural network you used to produce the final vector representation of each word and what are the mathematical functions used for the forward computation (i.e., from the pretrained word vectors to the final label of each word). Give the detailed setting of the network including which parameters are being updated, what are their sizes, and what is the length of the final vector representation of each word to be fed to the softmax classifier.\n",
    "\n",
    "(d) Report how many epochs you used for training, as well as the running time.\n",
    "\n",
    "(e) Report the accuracy on the test set, as well as the accuracy on the development set for each\n",
    "epoch during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:10:09.782156Z",
     "start_time": "2023-11-09T14:10:09.392532Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jeremy U\n",
      "[nltk_data]     Keat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4d9acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:11:50.085510Z",
     "start_time": "2023-11-09T14:10:09.582895Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446beddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:11:50.111556Z",
     "start_time": "2023-11-09T14:11:50.099575Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6900a97eff16ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code below tokenizes the modified csv file containing TREC dataset and proceeds to convert the tokens (words) into word2vec indexes. In addition, we format the labels to the correct input dimensions for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47079fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:39.460844Z",
     "start_time": "2023-11-09T14:16:39.393262Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e894b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.789718Z",
     "start_time": "2023-11-09T14:16:39.426597Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de5ed02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.940316Z",
     "start_time": "2023-11-09T14:16:40.793968Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.960904Z",
     "start_time": "2023-11-09T14:16:40.949478Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b525acea86ff3",
   "metadata": {},
   "source": [
    "The neural network used below is an LSTM-based network that predicts the sentence classification tags for each sentence. It consists of six main components:\n",
    "\n",
    "1. An embedding layer that maps each token to its embedding vector.\n",
    "2. An LSTM layer that processes the embedded tokens and produces LSTM outputs for each token.\n",
    "3. An aggregation function that summarizes each token output vector into one vector.\n",
    "4. Dropout layer for regularization purposes.\n",
    "5. Batch normalization layers for regularization purposes.\n",
    "6. Two fully connected layer (fc). The forward computation involves the following steps:\n",
    "\n",
    "Embedding                 : Mapping tokens to their embedding vectors using the embedding layer.  \n",
    "LSTM                      : Applying the LSTM on the embedded tokens, resulting in LSTM outputs for each token.  \n",
    "Reshaping                 : Making the output contiguous in memory and reshaping it for further processing.  \n",
    "Average pooling           : Computes average of each token output into one single word embedding.  \n",
    "Fully Connected Layers    : Applying the fully connected layers to obtain word embeddings before the softmax.  \n",
    "Log Softmax               : Applying log softmax to the output for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.978288Z",
     "start_time": "2023-11-09T14:16:40.965603Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.005) \n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 150)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(150)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(150, number_of_tags)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size, seq_len, embedding_dim = padded_output.size()\n",
    "        s = self.batch_norm1(padded_output.view(-1, embedding_dim))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        s = s.view(batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        s = torch.mean(s, dim=1)  # mean pooling\n",
    "        s = self.fc1(s)\n",
    "        s = self.batch_norm2(s)\n",
    "        s = self.relu(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc2(s)\n",
    "        out = self.batch_norm3(s)\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:35:26.007275Z",
     "start_time": "2023-11-09T14:35:25.993396Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:16:40.992519Z",
     "start_time": "2023-11-09T14:16:40.986802Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177fea0f40c3348",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Training and Eval code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T14:16:40.997010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    train_loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        train_loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(train_loss_avg()))\n",
    "    return train_loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    validation_loss_avg = RunningAverage()\n",
    "    validation_accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        validation_loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        validation_accuracy_avg.update(accuracy_val)\n",
    "\n",
    "    print(f\"{validation_loss_avg()=}\")\n",
    "    print(f\"{validation_accuracy_avg()=}\")\n",
    "    \n",
    "    return validation_loss_avg(), validation_accuracy_avg()\n",
    "\n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "\n",
    "    accuracies_across_epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        loss, accuracy = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "        accuracies_across_epochs.append(accuracy)\n",
    "\n",
    "        if early_stopper.early_stop(loss):             \n",
    "            break\n",
    "    \n",
    "    return accuracies_across_epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d13e94d4984c51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T14:16:41.017775Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.72it/s, loss=0.783] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.6101372718811036\n",
      "validation_accuracy_avg()=0.7979166666666667\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 122.27it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.42489729126294457\n",
      "validation_accuracy_avg()=0.8604166666666667\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 116.89it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3751857548952103\n",
      "validation_accuracy_avg()=0.8729166666666667\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 117.30it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.44155601461728416\n",
      "validation_accuracy_avg()=0.8729166666666667\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 124.21it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.42595240473747253\n",
      "validation_accuracy_avg()=0.8645833333333334\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 117.27it/s, loss=0.076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.39121079444885254\n",
      "validation_accuracy_avg()=0.86875\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 122.06it/s, loss=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.40027142961819967\n",
      "validation_accuracy_avg()=0.8791666666666667\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 112.58it/s, loss=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.35934528708457947\n",
      "validation_accuracy_avg()=0.8916666666666667\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 114.42it/s, loss=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.35651420752207436\n",
      "validation_accuracy_avg()=0.8895833333333333\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 113.51it/s, loss=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.352587553858757\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 116.21it/s, loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3626897434393565\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 108.98it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3521660258372625\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 112.73it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.35539696415265404\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 111.81it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.35636792580286664\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 111.26it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3601593762636185\n",
      "validation_accuracy_avg()=0.9041666666666667\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 114.68it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3633328328529994\n",
      "validation_accuracy_avg()=0.8979166666666667\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 112.92it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.36756266752878824\n",
      "validation_accuracy_avg()=0.9041666666666667\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 115.30it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.371139340599378\n",
      "validation_accuracy_avg()=0.9\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 114.24it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.37557741403579714\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 113.88it/s, loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.38028222918510435\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 117.63it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3838713437318802\n",
      "validation_accuracy_avg()=0.9041666666666667\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 117.51it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3887585868438085\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 111.44it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.39255629281202953\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 118.69it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.3970261186361313\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 101.28it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.40310676395893097\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 87.47it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4063160638014475\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 101.60it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4107486108938853\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 110.77it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.41550938189029696\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 115.97it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.42004578610261284\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 110.63it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4242940535147985\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 114.59it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4292596489191055\n",
      "validation_accuracy_avg()=0.9\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 108.74it/s, loss=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4331181784470876\n",
      "validation_accuracy_avg()=0.9020833333333333\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 111.19it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4365712672472\n",
      "validation_accuracy_avg()=0.9\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 107.82it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4416680465141932\n",
      "validation_accuracy_avg()=0.9\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 117.71it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.44481855432192485\n",
      "validation_accuracy_avg()=0.8979166666666667\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 114.22it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4502387374639511\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 119.11it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4559207300345103\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 116.07it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4598547379175822\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 116.61it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.463978902498881\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 109.80it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.4690431276957194\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 106.50it/s, loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss_avg()=0.47394442061583203\n",
      "validation_accuracy_avg()=0.8958333333333334\n",
      "execution_time=60.83588123321533\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "model = Net(embedding_weights, 300, 300, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "if (os.path.isfile(\"model_weights_average_pooling.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights_average_pooling.pth'))\n",
    "else:\n",
    "    start_time = time()\n",
    "    accuracies_across_epochs = train_and_evaluate(model, X_train_tokenized_indexified, y_train_formatted, X_val_tokenized_indexified, y_val_formatted, 100, 32, optimizer, loss_fn)\n",
    "    execution_time = time() - start_time\n",
    "    torch.save(model.state_dict(), 'model_weights_average_pooling.pth')\n",
    "    \n",
    "print(f\"{execution_time=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a182a6de64644",
   "metadata": {},
   "source": [
    "# Print out model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c392ff5d8891ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 torch.Size([1200, 300])\n",
      "lstm.weight_hh_l0 torch.Size([1200, 300])\n",
      "lstm.bias_ih_l0 torch.Size([1200])\n",
      "lstm.bias_hh_l0 torch.Size([1200])\n",
      "batch_norm1.weight torch.Size([300])\n",
      "batch_norm1.bias torch.Size([300])\n",
      "fc1.weight torch.Size([150, 300])\n",
      "fc1.bias torch.Size([150])\n",
      "batch_norm2.weight torch.Size([150])\n",
      "batch_norm2.bias torch.Size([150])\n",
      "fc2.weight torch.Size([5, 150])\n",
      "fc2.bias torch.Size([5])\n",
      "batch_norm3.weight torch.Size([5])\n",
      "batch_norm3.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69774dc9a35d44d8",
   "metadata": {},
   "source": [
    "## Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be8268f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:35:32.735076Z",
     "start_time": "2023-11-09T14:35:31.898900Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.922\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d221991c1181e13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T14:32:57.036402Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Development Set for Epoch 1: 0.7979\n",
      "Accuracy on Development Set for Epoch 2: 0.8604\n",
      "Accuracy on Development Set for Epoch 3: 0.8729\n",
      "Accuracy on Development Set for Epoch 4: 0.8729\n",
      "Accuracy on Development Set for Epoch 5: 0.8646\n",
      "Accuracy on Development Set for Epoch 6: 0.8688\n",
      "Accuracy on Development Set for Epoch 7: 0.8792\n",
      "Accuracy on Development Set for Epoch 8: 0.8917\n",
      "Accuracy on Development Set for Epoch 9: 0.8896\n",
      "Accuracy on Development Set for Epoch 10: 0.8958\n",
      "Accuracy on Development Set for Epoch 11: 0.9021\n",
      "Accuracy on Development Set for Epoch 12: 0.9021\n",
      "Accuracy on Development Set for Epoch 13: 0.9021\n",
      "Accuracy on Development Set for Epoch 14: 0.9021\n",
      "Accuracy on Development Set for Epoch 15: 0.9042\n",
      "Accuracy on Development Set for Epoch 16: 0.8979\n",
      "Accuracy on Development Set for Epoch 17: 0.9042\n",
      "Accuracy on Development Set for Epoch 18: 0.9000\n",
      "Accuracy on Development Set for Epoch 19: 0.9021\n",
      "Accuracy on Development Set for Epoch 20: 0.9021\n",
      "Accuracy on Development Set for Epoch 21: 0.9042\n",
      "Accuracy on Development Set for Epoch 22: 0.9021\n",
      "Accuracy on Development Set for Epoch 23: 0.9021\n",
      "Accuracy on Development Set for Epoch 24: 0.9021\n",
      "Accuracy on Development Set for Epoch 25: 0.9021\n",
      "Accuracy on Development Set for Epoch 26: 0.9021\n",
      "Accuracy on Development Set for Epoch 27: 0.9021\n",
      "Accuracy on Development Set for Epoch 28: 0.9021\n",
      "Accuracy on Development Set for Epoch 29: 0.9021\n",
      "Accuracy on Development Set for Epoch 30: 0.9021\n",
      "Accuracy on Development Set for Epoch 31: 0.9000\n",
      "Accuracy on Development Set for Epoch 32: 0.9021\n",
      "Accuracy on Development Set for Epoch 33: 0.9000\n",
      "Accuracy on Development Set for Epoch 34: 0.9000\n",
      "Accuracy on Development Set for Epoch 35: 0.8979\n",
      "Accuracy on Development Set for Epoch 36: 0.8958\n",
      "Accuracy on Development Set for Epoch 37: 0.8958\n",
      "Accuracy on Development Set for Epoch 38: 0.8958\n",
      "Accuracy on Development Set for Epoch 39: 0.8958\n",
      "Accuracy on Development Set for Epoch 40: 0.8958\n",
      "Accuracy on Development Set for Epoch 41: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# display accuracies on development set per epoch\n",
    "for epoch, accuracy in enumerate(accuracies_across_epochs):\n",
    "    print(f\"Accuracy on Development Set for Epoch {epoch + 1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T14:32:57.767799Z",
     "start_time": "2023-11-09T14:32:57.760799Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = What is a squirrel?, label = 0\n",
      "sentence = Is Singapore located in Southeast Asia?, label = 3\n",
      "sentence = Is Singapore in China?, label = 1\n",
      "sentence = Name 11 famous martyrs ., label = 4\n",
      "sentence = What ISPs exist in the Caribbean ?, label = 4\n",
      "sentence = How many cars are manufactured every day?, label = 4\n"
     ]
    }
   ],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia?\")\n",
    "print_sentence_label(\"Is Singapore in China?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
