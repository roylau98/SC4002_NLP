{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16fcb8704f34334f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:25:27.618021Z",
     "start_time": "2023-10-21T09:25:27.538076Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec_google_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:29.556240Z",
     "start_time": "2023-10-21T09:25:27.573881Z"
    }
   },
   "id": "af9bea8218da7d3b"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:38.684344Z",
     "start_time": "2023-10-21T09:26:38.670467Z"
    }
   },
   "outputs": [],
   "source": [
    "# words_path = \"stanford_cs230_medium_data/words.txt\"\n",
    "# words_path = \"stanford_cs230_small_data/words.txt\"\n",
    "# vocab = {}\n",
    "# with open(words_path, encoding=\"utf-8\") as f:\n",
    "#     for i, l in enumerate(f.read().splitlines()):\n",
    "#         vocab[l] = i\n",
    "\n",
    "vocab = word2vec_google_news.key_to_index\n",
    "vocab[\"<pad>\"] = 3_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8959b6d5bbdcb0e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:38.964765Z",
     "start_time": "2023-10-21T09:26:38.686631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'O': 0, 'I-geo': 1, 'I-gpe': 2, 'I-per': 3, 'I-org': 4, 'I-tim': 5}"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags_path = \"stanford_cs230_medium_data/tags.txt\"\n",
    "tags_path = \"stanford_cs230_small_data/tags.txt\"\n",
    "tag_map = {}\n",
    "with open(tags_path, encoding=\"utf-8\") as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        tag_map[l] = i\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cb4a69d3a6f4b99",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.001539Z",
     "start_time": "2023-10-21T09:26:38.771338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentences: [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307], [57, 1229, 2728, 3262, 336, 13, 16, 101276, 98307], [69307, 979, 1020, 5, 11, 3499, 11357, 273, 1, 98307, 496, 98307, 68, 98307, 1238, 1009, 98307], [73, 1275, 1080, 229, 24, 266, 5, 2125, 618, 98307, 366, 869, 5, 12054, 98307, 29255, 4483, 4013, 98307, 677, 1275, 1080, 8622, 98307], [7, 17027, 14046, 2599, 2, 12054, 9, 247, 17, 11, 954, 98307, 7106, 2125, 8767, 349, 28, 116, 130, 1227, 65, 1353, 1, 2821, 8, 12054, 98307], [51, 9, 618, 130, 1227, 2353, 98307, 522, 2196, 5, 12054, 18, 11, 1217, 8767, 2799, 792, 98307], [7, 229, 53, 2128, 14046, 618, 98307, 6446, 11, 131, 371, 601, 1215, 18, 12054, 98307, 467, 3181, 98307], [7, 1080, 229, 11092, 12054, 98307, 121, 98307, 3327, 1, 2039, 914, 10708, 98307, 1275, 1080, 8622, 98307, 755, 103, 78, 5336, 756, 1, 12054, 98307], [1290, 247, 159, 7144, 21, 496, 98307, 536, 98307, 11, 9330, 2434, 363, 18, 249, 1290, 4018, 1704, 172190, 20874, 98307], [2628, 159, 673257, 98307, 10, 441, 252, 55, 7144, 9967, 26, 124, 420, 11, 2150, 219, 98307, 12005, 98307]]\n",
      "train_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# train_sentences_file = \"stanford_cs230_medium_data/train/sentences.txt\"\n",
    "# train_labels_file = \"stanford_cs230_medium_data/train/labels.txt\"\n",
    "train_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "train_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "with open(train_sentences_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        train_sentences.append(s)\n",
    "\n",
    "with open(train_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        train_labels.append(l)\n",
    "        \n",
    "print(f\"train_sentences: {train_sentences}\")\n",
    "print(f\"train_labels: {train_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "736b1f18b861288f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.036969Z",
     "start_time": "2023-10-21T09:26:38.803035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_sentences: [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307], [57, 1229, 2728, 3262, 336, 13, 16, 101276, 98307], [69307, 979, 1020, 5, 11, 3499, 11357, 273, 1, 98307, 496, 98307, 68, 98307, 1238, 1009, 98307], [73, 1275, 1080, 229, 24, 266, 5, 2125, 618, 98307, 366, 869, 5, 12054, 98307, 29255, 4483, 4013, 98307, 677, 1275, 1080, 8622, 98307], [7, 17027, 14046, 2599, 2, 12054, 9, 247, 17, 11, 954, 98307, 7106, 2125, 8767, 349, 28, 116, 130, 1227, 65, 1353, 1, 2821, 8, 12054, 98307], [51, 9, 618, 130, 1227, 2353, 98307, 522, 2196, 5, 12054, 18, 11, 1217, 8767, 2799, 792, 98307], [7, 229, 53, 2128, 14046, 618, 98307, 6446, 11, 131, 371, 601, 1215, 18, 12054, 98307, 467, 3181, 98307], [7, 1080, 229, 11092, 12054, 98307, 121, 98307, 3327, 1, 2039, 914, 10708, 98307, 1275, 1080, 8622, 98307, 755, 103, 78, 5336, 756, 1, 12054, 98307], [1290, 247, 159, 7144, 21, 496, 98307, 536, 98307, 11, 9330, 2434, 363, 18, 249, 1290, 4018, 1704, 172190, 20874, 98307], [2628, 159, 673257, 98307, 10, 441, 252, 55, 7144, 9967, 26, 124, 420, 11, 2150, 219, 98307, 12005, 98307]]\n",
      "val_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# val_sentences_file = \"stanford_cs230_medium_data/val/sentences.txt\"\n",
    "# val_labels_file = \"stanford_cs230_medium_data/val/labels.txt\"\n",
    "val_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "val_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "with open(val_sentences_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        val_sentences.append(s)\n",
    "\n",
    "with open(val_labels_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        val_labels.append(l)\n",
    "\n",
    "print(f\"val_sentences: {val_sentences}\")\n",
    "print(f\"val_labels: {val_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.174159Z",
     "start_time": "2023-10-21T09:26:38.862702Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        # if params.cuda:\n",
    "        #     batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.193489Z",
     "start_time": "2023-10-21T09:26:39.048853Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
    "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
    "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
    "    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n",
    "\n",
    "    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n",
    "    you can go about defining your own network.\n",
    "\n",
    "    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        \"\"\"\n",
    "        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n",
    "        required are:\n",
    "\n",
    "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "\n",
    "        Args:\n",
    "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, padding_idx=3_000_000)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        # for more details on how to use it, check out the documentation\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This function defines how we use the components of our network to operate on an input batch.\n",
    "\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "        s, _ = self.lstm(s)\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "        s = s.view(-1, s.shape[2])\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.193878Z",
     "start_time": "2023-10-21T09:26:39.071576Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e07330c216283eda",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.194810Z",
     "start_time": "2023-10-21T09:26:39.090892Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84b8f1d154a50053",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.196012Z",
     "start_time": "2023-10-21T09:26:39.110724Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % 10 == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1feee96f562ef39",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.220440Z",
     "start_time": "2023-10-21T09:26:39.131080Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "798027bbaa11f757",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.221159Z",
     "start_time": "2023-10-21T09:26:39.145200Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        metrics\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(\n",
    "            train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator,\n",
    "              metrics, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(\n",
    "            val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        val_metrics = evaluate(\n",
    "            model, loss_fn, val_data_iterator, metrics, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a981f205b1133b82",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.250463Z",
     "start_time": "2023-10-21T09:26:39.170710Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3000000 is out of bounds for axis 0 with size 3000000",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[82], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mword2vec_google_news\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<pad>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m300\u001B[39m)\n\u001B[1;32m      2\u001B[0m embedding_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(word2vec_google_news\u001B[38;5;241m.\u001B[39mvectors)\n",
      "File \u001B[0;32m~/IdeaProjects/SC4002_NLP/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:635\u001B[0m, in \u001B[0;36mKeyedVectors.__setitem__\u001B[0;34m(self, keys, weights)\u001B[0m\n\u001B[1;32m    632\u001B[0m     keys \u001B[38;5;241m=\u001B[39m [keys]\n\u001B[1;32m    633\u001B[0m     weights \u001B[38;5;241m=\u001B[39m weights\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 635\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_vectors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/IdeaProjects/SC4002_NLP/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:612\u001B[0m, in \u001B[0;36mKeyedVectors.add_vectors\u001B[0;34m(self, keys, weights, extras, replace)\u001B[0m\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m replace:\n\u001B[1;32m    611\u001B[0m     in_vocab_idxs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_index(keys[idx]) \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnonzero(in_vocab_mask)[\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m--> 612\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectors\u001B[49m\u001B[43m[\u001B[49m\u001B[43min_vocab_idxs\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m weights[in_vocab_mask]\n\u001B[1;32m    613\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attr, extra \u001B[38;5;129;01min\u001B[39;00m extras:\n\u001B[1;32m    614\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpandos[attr][in_vocab_idxs] \u001B[38;5;241m=\u001B[39m extra[in_vocab_mask]\n",
      "\u001B[0;31mIndexError\u001B[0m: index 3000000 is out of bounds for axis 0 with size 3000000"
     ]
    }
   ],
   "source": [
    "word2vec_google_news[\"<pad>\"] = np.zeros(300)\n",
    "embedding_weights = torch.FloatTensor(word2vec_google_news.vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.357632Z",
     "start_time": "2023-10-21T09:26:39.197730Z"
    }
   },
   "id": "99218b509d3b1af1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.353038Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually change vocab size (unique no. of words) and change label size (unique no. of labels) for now\n",
    "model = Net(embedding_weights, 50, 50, 6)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_and_evaluate(model, train_sentences, train_labels, val_sentences, val_labels, 100, 5, optimizer, loss_fn, metrics)\n",
    "\n",
    "if (os.path.isfile(\"model_weights.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights.pth'))\n",
    "else:\n",
    "    torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5724d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.372790Z",
     "start_time": "2023-10-21T09:26:54.365071Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_sentences_file = \"stanford_cs230_medium_data/test/sentences.txt\"\n",
    "# test_labels_file = \"stanford_cs230_medium_data/test/labels.txt\"\n",
    "test_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "test_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "sentences_w_words = []\n",
    "labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "with open(test_sentences_file) as f:\n",
    "    for sentence_words in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentences_w_words.append(sentence_words)\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence_words.split(' ')]\n",
    "        test_sentences.append(s)\n",
    "\n",
    "with open(test_labels_file) as f:\n",
    "    for sentence_labels in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence_labels.split(' ')]\n",
    "        test_labels.append(l)\n",
    "        labels.append(sentence_labels)\n",
    "\n",
    "print(f\"sentences_w_words: {sentences_w_words}\")\n",
    "print(f\"labels: {labels}\")\n",
    "print(f\"test_sentences: {test_sentences}\")\n",
    "print(f\"test_labels: {test_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8268f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.368968Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(test_sentences))\n",
    "test_data_iterator = data_iterator(test_sentences, test_labels, len(test_sentences), 1, shuffle=False)\n",
    "test_batch, labels_batch, abc = next(test_data_iterator)\n",
    "model_output = model(test_batch)\n",
    "print(model_output)\n",
    "print(model_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42671f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.375040Z",
     "start_time": "2023-10-21T09:26:54.373552Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"model_output \\n {model_output}\")\n",
    "# predicted_labels = torch.argmax(model_output, dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# predicted_labels = torch.argmax(torch.abs(model_output), dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "predicted_labels = np.argmax(model_output.detach().numpy(), axis=1)\n",
    "print(f\"sentences_w_words \\n {abc}\")\n",
    "print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# print(f\"correct_labels \\n {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ad35f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.378099Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
