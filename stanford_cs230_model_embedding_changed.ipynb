{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:25:27.618021Z",
     "start_time": "2023-10-21T09:25:27.538076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9bea8218da7d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:29.556240Z",
     "start_time": "2023-10-21T09:25:27.573881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec_google_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:38.684344Z",
     "start_time": "2023-10-21T09:26:38.670467Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_google_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "padding_idx = word2vec_google_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_google_news.vectors)\n",
    "vocab = word2vec_google_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4542beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"padding_idx: {padding_idx}\")\n",
    "# print(len(embedding_weights))\n",
    "# print(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8959b6d5bbdcb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:38.964765Z",
     "start_time": "2023-10-21T09:26:38.686631Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'I-geo': 1, 'I-gpe': 2, 'I-per': 3, 'I-org': 4, 'I-tim': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags_path = \"stanford_cs230_medium_data/tags.txt\"\n",
    "tags_path = \"stanford_cs230_small_data/tags.txt\"\n",
    "tag_map = {}\n",
    "with open(tags_path, encoding=\"utf-8\") as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        tag_map[l] = i\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb4a69d3a6f4b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.001539Z",
     "start_time": "2023-10-21T09:26:38.771338Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentences: [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307], [57, 1229, 2728, 3262, 336, 13, 16, 101276, 98307], [69307, 979, 1020, 5, 11, 3499, 11357, 273, 1, 98307, 496, 98307, 68, 98307, 1238, 1009, 98307], [73, 1275, 1080, 229, 24, 266, 5, 2125, 618, 98307, 366, 869, 5, 12054, 98307, 29255, 4483, 4013, 98307, 677, 1275, 1080, 8622, 98307], [7, 17027, 14046, 2599, 2, 12054, 9, 247, 17, 11, 954, 98307, 7106, 2125, 8767, 349, 28, 116, 130, 1227, 65, 1353, 1, 2821, 8, 12054, 98307], [51, 9, 618, 130, 1227, 2353, 98307, 522, 2196, 5, 12054, 18, 11, 1217, 8767, 2799, 792, 98307], [7, 229, 53, 2128, 14046, 618, 98307, 6446, 11, 131, 371, 601, 1215, 18, 12054, 98307, 467, 3181, 98307], [7, 1080, 229, 11092, 12054, 98307, 121, 98307, 3327, 1, 2039, 914, 10708, 98307, 1275, 1080, 8622, 98307, 755, 103, 78, 5336, 756, 1, 12054, 98307], [1290, 247, 159, 7144, 21, 496, 98307, 536, 98307, 11, 9330, 2434, 363, 18, 249, 1290, 4018, 1704, 172190, 20874, 98307], [2628, 159, 673257, 98307, 10, 441, 252, 55, 7144, 9967, 26, 124, 420, 11, 2150, 219, 98307, 12005, 98307]]\n",
      "train_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# train_sentences_file = \"stanford_cs230_medium_data/train/sentences.txt\"\n",
    "# train_labels_file = \"stanford_cs230_medium_data/train/labels.txt\"\n",
    "train_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "train_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "with open(train_sentences_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        train_sentences.append(s)\n",
    "\n",
    "with open(train_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        train_labels.append(l)\n",
    "        \n",
    "print(f\"train_sentences: {train_sentences}\")\n",
    "print(f\"train_labels: {train_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736b1f18b861288f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.036969Z",
     "start_time": "2023-10-21T09:26:38.803035Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_sentences: [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307], [57, 1229, 2728, 3262, 336, 13, 16, 101276, 98307], [69307, 979, 1020, 5, 11, 3499, 11357, 273, 1, 98307, 496, 98307, 68, 98307, 1238, 1009, 98307], [73, 1275, 1080, 229, 24, 266, 5, 2125, 618, 98307, 366, 869, 5, 12054, 98307, 29255, 4483, 4013, 98307, 677, 1275, 1080, 8622, 98307], [7, 17027, 14046, 2599, 2, 12054, 9, 247, 17, 11, 954, 98307, 7106, 2125, 8767, 349, 28, 116, 130, 1227, 65, 1353, 1, 2821, 8, 12054, 98307], [51, 9, 618, 130, 1227, 2353, 98307, 522, 2196, 5, 12054, 18, 11, 1217, 8767, 2799, 792, 98307], [7, 229, 53, 2128, 14046, 618, 98307, 6446, 11, 131, 371, 601, 1215, 18, 12054, 98307, 467, 3181, 98307], [7, 1080, 229, 11092, 12054, 98307, 121, 98307, 3327, 1, 2039, 914, 10708, 98307, 1275, 1080, 8622, 98307, 755, 103, 78, 5336, 756, 1, 12054, 98307], [1290, 247, 159, 7144, 21, 496, 98307, 536, 98307, 11, 9330, 2434, 363, 18, 249, 1290, 4018, 1704, 172190, 20874, 98307], [2628, 159, 673257, 98307, 10, 441, 252, 55, 7144, 9967, 26, 124, 420, 11, 2150, 219, 98307, 12005, 98307]]\n",
      "val_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# val_sentences_file = \"stanford_cs230_medium_data/val/sentences.txt\"\n",
    "# val_labels_file = \"stanford_cs230_medium_data/val/labels.txt\"\n",
    "val_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "val_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "with open(val_sentences_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        val_sentences.append(s)\n",
    "\n",
    "with open(val_labels_file, encoding=\"utf-8\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        val_labels.append(l)\n",
    "\n",
    "print(f\"val_sentences: {val_sentences}\")\n",
    "print(f\"val_labels: {val_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.174159Z",
     "start_time": "2023-10-21T09:26:38.862702Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        # if params.cuda:\n",
    "        #     batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.193489Z",
     "start_time": "2023-10-21T09:26:39.048853Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
    "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
    "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
    "    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n",
    "\n",
    "    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n",
    "    you can go about defining your own network.\n",
    "\n",
    "    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        \"\"\"\n",
    "        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n",
    "        required are:\n",
    "\n",
    "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "\n",
    "        Args:\n",
    "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, padding_idx=3_000_000)\n",
    "        # self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        # for more details on how to use it, check out the documentation\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This function defines how we use the components of our network to operate on an input batch.\n",
    "\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "        s, _ = self.lstm(s)\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "        s = s.view(-1, s.shape[2])\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.193878Z",
     "start_time": "2023-10-21T09:26:39.071576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.194810Z",
     "start_time": "2023-10-21T09:26:39.090892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.196012Z",
     "start_time": "2023-10-21T09:26:39.110724Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % 10 == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1feee96f562ef39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.220440Z",
     "start_time": "2023-10-21T09:26:39.131080Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798027bbaa11f757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.221159Z",
     "start_time": "2023-10-21T09:26:39.145200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        metrics\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(\n",
    "            train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator,\n",
    "              metrics, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(\n",
    "            val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        val_metrics = evaluate(\n",
    "            model, loss_fn, val_data_iterator, metrics, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a981f205b1133b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:39.250463Z",
     "start_time": "2023-10-21T09:26:39.170710Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99218b509d3b1af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.357632Z",
     "start_time": "2023-10-21T09:26:39.197730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_google_news[\"<pad>\"] = np.zeros(300)\n",
    "embedding_weights = torch.FloatTensor(word2vec_google_news.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.353038Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 66.67it/s, loss=1.754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.106 ; loss: 1.781\n",
      "- Eval metrics : accuracy: 0.797 ; loss: 1.631\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.87it/s, loss=1.590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 1.599\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 1.422\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=1.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 1.353\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 1.057\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=1.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.917\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.992\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.755\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.955\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.696\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.851\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.631\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.808\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.621\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.798\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.627\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.771\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.603\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.734\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.68it/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.560\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.705\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.82it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.520\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.686\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.492\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.667\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.471\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.640\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.451\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.608\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.84it/s, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.434\n",
      "- Eval metrics : accuracy: 0.802 ; loss: 0.581\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.422\n",
      "- Eval metrics : accuracy: 0.807 ; loss: 0.556\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.65it/s, loss=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.407\n",
      "- Eval metrics : accuracy: 0.807 ; loss: 0.524\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.377\n",
      "- Eval metrics : accuracy: 0.807 ; loss: 0.490\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.894 ; loss: 0.344\n",
      "- Eval metrics : accuracy: 0.837 ; loss: 0.459\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.904 ; loss: 0.323\n",
      "- Eval metrics : accuracy: 0.871 ; loss: 0.428\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.915 ; loss: 0.306\n",
      "- Eval metrics : accuracy: 0.881 ; loss: 0.395\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.915 ; loss: 0.280\n",
      "- Eval metrics : accuracy: 0.886 ; loss: 0.361\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.926 ; loss: 0.252\n",
      "- Eval metrics : accuracy: 0.906 ; loss: 0.330\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.88it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.947 ; loss: 0.234\n",
      "- Eval metrics : accuracy: 0.921 ; loss: 0.299\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.957 ; loss: 0.216\n",
      "- Eval metrics : accuracy: 0.921 ; loss: 0.272\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.947 ; loss: 0.200\n",
      "- Eval metrics : accuracy: 0.941 ; loss: 0.244\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.81it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.968 ; loss: 0.185\n",
      "- Eval metrics : accuracy: 0.936 ; loss: 0.220\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.968 ; loss: 0.166\n",
      "- Eval metrics : accuracy: 0.956 ; loss: 0.198\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.968 ; loss: 0.151\n",
      "- Eval metrics : accuracy: 0.965 ; loss: 0.179\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.979 ; loss: 0.137\n",
      "- Eval metrics : accuracy: 0.970 ; loss: 0.162\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.84it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.979 ; loss: 0.127\n",
      "- Eval metrics : accuracy: 0.975 ; loss: 0.146\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.115\n",
      "- Eval metrics : accuracy: 0.970 ; loss: 0.130\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.979 ; loss: 0.103\n",
      "- Eval metrics : accuracy: 0.975 ; loss: 0.115\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.090\n",
      "- Eval metrics : accuracy: 0.980 ; loss: 0.102\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.080\n",
      "- Eval metrics : accuracy: 0.985 ; loss: 0.092\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.076\n",
      "- Eval metrics : accuracy: 0.990 ; loss: 0.080\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.67it/s, loss=0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.066\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.071\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.81it/s, loss=0.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.059\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.064\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.053\n",
      "- Eval metrics : accuracy: 0.995 ; loss: 0.058\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.989 ; loss: 0.050\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.052\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.52it/s, loss=0.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.044\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.046\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 147.96it/s, loss=0.045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.040\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.042\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 133.33it/s, loss=0.040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.036\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.038\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.033\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.035\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.030\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.031\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.027\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.028\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.024\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.026\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.023\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.024\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.021\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.022\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.019\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.021\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.88it/s, loss=0.020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.018\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.019\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.017\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.018\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.016\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.017\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.87it/s, loss=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.015\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.016\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.66it/s, loss=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.014\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.015\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.013\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.014\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.82it/s, loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.012\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.013\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.012\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.012\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.011\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.012\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.010\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.011\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.010\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.011\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.009\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.010\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.009\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.010\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.008\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.009\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.65it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.008\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.009\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.008\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.009\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.007\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.008\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.007\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.008\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.87it/s, loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.007\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.008\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.007\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.007\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.006\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.007\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.006\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.007\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.65it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.006\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.007\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.006\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.006\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.006\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.006\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.006\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.006\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.84it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.005\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.83it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.87it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.82it/s, loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.005\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 147.65it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 159.81it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.004\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.68it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 159.76it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.86it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.004\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.86it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.003\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 142.85it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.003\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.85it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.003\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 166.68it/s, loss=0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 1.000 ; loss: 0.003\n",
      "- Eval metrics : accuracy: 1.000 ; loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "# manually change vocab size (unique no. of words) and change label size (unique no. of labels) for now\n",
    "model = Net(embedding_weights, 300, 300, 6)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_and_evaluate(model, train_sentences, train_labels, val_sentences, val_labels, 100, 5, optimizer, loss_fn, metrics)\n",
    "\n",
    "if (os.path.isfile(\"model_weights.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights.pth'))\n",
    "else:\n",
    "    torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbc5724d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.372790Z",
     "start_time": "2023-10-21T09:26:54.365071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_w_words: ['At the Group of Eight summit in Scotland , Japanese Prime Minister Junichiro Koizumi said he is outraged by the London attacks .', 'He noted terrorist acts must not be forgivable .', 'Sarin gas attacks on the Tokyo subway system in 1995 killed 12 people and injured thousands .', 'A human rights group has called on Asian leaders to increase pressure on Burma to hasten democratic reforms and stop human rights abuses .', 'The Alternative ASEAN Network for Burma said officials from the Association of Southeast Asian Nations meeting this week should consider new options in dealing with Burma .', 'It said leaders should consider supporting a possible resolution on Burma by the United Nations Security Council .', \"The group also urged ASEAN leaders to acknowledge the many security problems caused by Burma 's military regime .\", \"The rights group accuses Burma 's government of involvement in illegal drug trafficking and human rights abuses , especially against some ethnic groups in Burma .\", 'Iraqi officials say gunmen have killed a member of the secular coalition led by former Iraqi prime minister Ayad Allawi .', 'Officials say Faras al-Jabouri was shot Saturday after gunmen raided his home near the northern city of Mosul .']\n",
      "labels: ['O O I-org I-org I-org I-org O I-geo O I-gpe I-per I-per I-per I-per O O O O O O I-geo O O', 'O O O O O O O O O', 'O O O O O I-geo O O O I-tim O O O O O O O', 'O O O O O O O O O O O O O I-geo O O O O O O O O O O', 'O I-org I-org I-org I-org I-org O O O O I-org I-org I-org I-org I-org O O O O O O O O O O I-geo O', 'O O O O O O O O O O I-geo O O I-org I-org I-org I-org O', 'O O O O I-org O O O O O O O O O I-geo O O O O', 'O O O O I-geo O O O O O O O O O O O O O O O O O O O I-geo O', 'I-gpe O O O O O O O O O O O O O O I-gpe O O I-per I-per O', 'O O O O O O I-tim O O O O O O O O O O I-geo O']\n",
      "test_sentences: [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307], [57, 1229, 2728, 3262, 336, 13, 16, 101276, 98307], [69307, 979, 1020, 5, 11, 3499, 11357, 273, 1, 98307, 496, 98307, 68, 98307, 1238, 1009, 98307], [73, 1275, 1080, 229, 24, 266, 5, 2125, 618, 98307, 366, 869, 5, 12054, 98307, 29255, 4483, 4013, 98307, 677, 1275, 1080, 8622, 98307], [7, 17027, 14046, 2599, 2, 12054, 9, 247, 17, 11, 954, 98307, 7106, 2125, 8767, 349, 28, 116, 130, 1227, 65, 1353, 1, 2821, 8, 12054, 98307], [51, 9, 618, 130, 1227, 2353, 98307, 522, 2196, 5, 12054, 18, 11, 1217, 8767, 2799, 792, 98307], [7, 229, 53, 2128, 14046, 618, 98307, 6446, 11, 131, 371, 601, 1215, 18, 12054, 98307, 467, 3181, 98307], [7, 1080, 229, 11092, 12054, 98307, 121, 98307, 3327, 1, 2039, 914, 10708, 98307, 1275, 1080, 8622, 98307, 755, 103, 78, 5336, 756, 1, 12054, 98307], [1290, 247, 159, 7144, 21, 496, 98307, 536, 98307, 11, 9330, 2434, 363, 18, 249, 1290, 4018, 1704, 172190, 20874, 98307], [2628, 159, 673257, 98307, 10, 441, 252, 55, 7144, 9967, 26, 124, 420, 11, 2150, 219, 98307, 12005, 98307]]\n",
      "test_labels: [[0, 0, 4, 4, 4, 4, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 4, 4, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# test_sentences_file = \"stanford_cs230_medium_data/test/sentences.txt\"\n",
    "# test_labels_file = \"stanford_cs230_medium_data/test/labels.txt\"\n",
    "test_sentences_file = \"stanford_cs230_small_data/test/sentences.txt\"\n",
    "test_labels_file = \"stanford_cs230_small_data/test/labels.txt\"\n",
    "\n",
    "sentences_w_words = []\n",
    "labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "with open(test_sentences_file) as f:\n",
    "    for sentence_words in f.read().splitlines():\n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentences_w_words.append(sentence_words)\n",
    "        s = [vocab[token] if token in vocab\n",
    "             else vocab['UNK']\n",
    "             for token in sentence_words.split(' ')]\n",
    "        test_sentences.append(s)\n",
    "\n",
    "with open(test_labels_file) as f:\n",
    "    for sentence_labels in f.read().splitlines():\n",
    "        #replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence_labels.split(' ')]\n",
    "        test_labels.append(l)\n",
    "        labels.append(sentence_labels)\n",
    "\n",
    "print(f\"sentences_w_words: {sentences_w_words}\")\n",
    "print(f\"labels: {labels}\")\n",
    "print(f\"test_sentences: {test_sentences}\")\n",
    "print(f\"test_labels: {test_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be8268f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.368968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([[-1.9503e-02, -5.7721e+00, -6.5976e+00, -6.1075e+00, -4.5781e+00,\n",
      "         -6.0591e+00],\n",
      "        [-6.6907e-03, -6.9312e+00, -9.5465e+00, -8.9604e+00, -5.2664e+00,\n",
      "         -8.0176e+00],\n",
      "        [-5.1182e+00, -5.3215e+00, -9.3186e+00, -8.4371e+00, -1.2099e-02,\n",
      "         -7.0733e+00],\n",
      "        [-7.9006e+00, -8.3355e+00, -1.0205e+01, -9.6033e+00, -1.2172e-03,\n",
      "         -7.5976e+00],\n",
      "        [-7.1119e+00, -8.3138e+00, -9.9251e+00, -9.3340e+00, -1.3209e-03,\n",
      "         -9.0097e+00],\n",
      "        [-4.4683e+00, -6.4885e+00, -8.7820e+00, -8.1045e+00, -1.3622e-02,\n",
      "         -9.3646e+00],\n",
      "        [-9.1961e-03, -6.8682e+00, -9.8850e+00, -9.7032e+00, -4.8316e+00,\n",
      "         -1.0488e+01],\n",
      "        [-4.4696e+00, -2.3224e-02, -4.9887e+00, -8.6717e+00, -5.4459e+00,\n",
      "         -8.4939e+00],\n",
      "        [-5.7030e-03, -6.2460e+00, -6.0090e+00, -8.4167e+00, -7.4359e+00,\n",
      "         -7.6393e+00],\n",
      "        [-6.3135e+00, -4.8082e+00, -2.4245e-02, -4.4485e+00, -7.4930e+00,\n",
      "         -6.3617e+00],\n",
      "        [-5.6175e+00, -8.4825e+00, -4.5107e+00, -1.6976e-02, -6.7134e+00,\n",
      "         -7.1480e+00],\n",
      "        [-7.8673e+00, -1.0297e+01, -7.2034e+00, -1.4714e-03, -8.4498e+00,\n",
      "         -9.2556e+00],\n",
      "        [-6.8987e+00, -9.4059e+00, -6.9901e+00, -2.4053e-03, -8.2332e+00,\n",
      "         -8.9913e+00],\n",
      "        [-5.2594e+00, -8.4515e+00, -6.1185e+00, -8.1216e-03, -7.8467e+00,\n",
      "         -9.3870e+00],\n",
      "        [-2.4962e-03, -1.1146e+01, -1.1028e+01, -6.0187e+00, -1.0747e+01,\n",
      "         -1.1721e+01],\n",
      "        [-8.5830e-06, -1.3828e+01, -1.5744e+01, -1.1880e+01, -1.4765e+01,\n",
      "         -1.5331e+01],\n",
      "        [-3.0994e-06, -1.3025e+01, -1.6937e+01, -1.4399e+01, -1.5507e+01,\n",
      "         -1.5841e+01],\n",
      "        [-1.0610e-05, -1.1565e+01, -1.6572e+01, -1.4797e+01, -1.4991e+01,\n",
      "         -1.5224e+01],\n",
      "        [-6.2106e-05, -9.7384e+00, -1.5557e+01, -1.4212e+01, -1.3706e+01,\n",
      "         -1.3677e+01],\n",
      "        [-2.3887e-03, -6.0750e+00, -1.2985e+01, -1.1600e+01, -1.0498e+01,\n",
      "         -9.9611e+00],\n",
      "        [-5.6956e+00, -5.2743e-03, -7.9134e+00, -9.5765e+00, -7.4917e+00,\n",
      "         -7.0054e+00],\n",
      "        [-4.3192e-04, -7.8490e+00, -1.2847e+01, -1.2368e+01, -1.0930e+01,\n",
      "         -1.0987e+01],\n",
      "        [-1.8619e-04, -9.1650e+00, -1.4135e+01, -1.3398e+01, -1.0592e+01,\n",
      "         -9.8247e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([23, 6])\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))\n",
    "test_data_iterator = data_iterator(test_sentences, test_labels, len(test_sentences), 1, shuffle=False)\n",
    "test_batch, labels_batch, abc = next(test_data_iterator)\n",
    "model_output = model(test_batch)\n",
    "print(model_output)\n",
    "print(model_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff42671f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T09:26:54.375040Z",
     "start_time": "2023-10-21T09:26:54.373552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output \n",
      " tensor([[-1.9503e-02, -5.7721e+00, -6.5976e+00, -6.1075e+00, -4.5781e+00,\n",
      "         -6.0591e+00],\n",
      "        [-6.6907e-03, -6.9312e+00, -9.5465e+00, -8.9604e+00, -5.2664e+00,\n",
      "         -8.0176e+00],\n",
      "        [-5.1182e+00, -5.3215e+00, -9.3186e+00, -8.4371e+00, -1.2099e-02,\n",
      "         -7.0733e+00],\n",
      "        [-7.9006e+00, -8.3355e+00, -1.0205e+01, -9.6033e+00, -1.2172e-03,\n",
      "         -7.5976e+00],\n",
      "        [-7.1119e+00, -8.3138e+00, -9.9251e+00, -9.3340e+00, -1.3209e-03,\n",
      "         -9.0097e+00],\n",
      "        [-4.4683e+00, -6.4885e+00, -8.7820e+00, -8.1045e+00, -1.3622e-02,\n",
      "         -9.3646e+00],\n",
      "        [-9.1961e-03, -6.8682e+00, -9.8850e+00, -9.7032e+00, -4.8316e+00,\n",
      "         -1.0488e+01],\n",
      "        [-4.4696e+00, -2.3224e-02, -4.9887e+00, -8.6717e+00, -5.4459e+00,\n",
      "         -8.4939e+00],\n",
      "        [-5.7030e-03, -6.2460e+00, -6.0090e+00, -8.4167e+00, -7.4359e+00,\n",
      "         -7.6393e+00],\n",
      "        [-6.3135e+00, -4.8082e+00, -2.4245e-02, -4.4485e+00, -7.4930e+00,\n",
      "         -6.3617e+00],\n",
      "        [-5.6175e+00, -8.4825e+00, -4.5107e+00, -1.6976e-02, -6.7134e+00,\n",
      "         -7.1480e+00],\n",
      "        [-7.8673e+00, -1.0297e+01, -7.2034e+00, -1.4714e-03, -8.4498e+00,\n",
      "         -9.2556e+00],\n",
      "        [-6.8987e+00, -9.4059e+00, -6.9901e+00, -2.4053e-03, -8.2332e+00,\n",
      "         -8.9913e+00],\n",
      "        [-5.2594e+00, -8.4515e+00, -6.1185e+00, -8.1216e-03, -7.8467e+00,\n",
      "         -9.3870e+00],\n",
      "        [-2.4962e-03, -1.1146e+01, -1.1028e+01, -6.0187e+00, -1.0747e+01,\n",
      "         -1.1721e+01],\n",
      "        [-8.5830e-06, -1.3828e+01, -1.5744e+01, -1.1880e+01, -1.4765e+01,\n",
      "         -1.5331e+01],\n",
      "        [-3.0994e-06, -1.3025e+01, -1.6937e+01, -1.4399e+01, -1.5507e+01,\n",
      "         -1.5841e+01],\n",
      "        [-1.0610e-05, -1.1565e+01, -1.6572e+01, -1.4797e+01, -1.4991e+01,\n",
      "         -1.5224e+01],\n",
      "        [-6.2106e-05, -9.7384e+00, -1.5557e+01, -1.4212e+01, -1.3706e+01,\n",
      "         -1.3677e+01],\n",
      "        [-2.3887e-03, -6.0750e+00, -1.2985e+01, -1.1600e+01, -1.0498e+01,\n",
      "         -9.9611e+00],\n",
      "        [-5.6956e+00, -5.2743e-03, -7.9134e+00, -9.5765e+00, -7.4917e+00,\n",
      "         -7.0054e+00],\n",
      "        [-4.3192e-04, -7.8490e+00, -1.2847e+01, -1.2368e+01, -1.0930e+01,\n",
      "         -1.0987e+01],\n",
      "        [-1.8619e-04, -9.1650e+00, -1.4135e+01, -1.3398e+01, -1.0592e+01,\n",
      "         -9.8247e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "sentences_w_words \n",
      " [[332, 11, 708, 98307, 6318, 3211, 1, 3067, 98307, 1759, 15201, 2031, 866354, 22076, 9, 22, 4, 12597, 18, 11, 791, 1020, 98307]]\n",
      "predicted_labels \n",
      " [0 0 4 4 4 4 0 1 0 2 3 3 3 3 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_output \\n {model_output}\")\n",
    "# predicted_labels = torch.argmax(model_output, dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# predicted_labels = torch.argmax(torch.abs(model_output), dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "predicted_labels = np.argmax(model_output.detach().numpy(), axis=1)\n",
    "print(f\"sentences_w_words \\n {abc}\")\n",
    "print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# print(f\"correct_labels \\n {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ad35f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-21T09:26:54.378099Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
