{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import gensim.downloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        # tag_scores = F.softmax(tag_space, dim=-1) # which library is F from?\n",
    "        return tag_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 64\n",
    "VOCAB_SIZE = 3000000\n",
    "TARGET_SIZE = 3000000\n",
    "learning_rate = 0.001\n",
    "model = LSTMTagger(EMBEDDING_DIM,HIDDEN_DIM,VOCAB_SIZE,TARGET_SIZE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):\n",
    "    df_train, df_test = train_test_split(df, test_size=0.3, random_state=None)\n",
    "    labels_train = df_train[\"tags\"]\n",
    "    df_train = df_train.drop(df_train[\"tags\"])\n",
    "    labels_test = df_test[\"tags\"]\n",
    "    df_test = df_test.drop(df_test[\"tags\"])\n",
    "\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    df_train_scaled = standard_scaler.fit_transform(df_train)\n",
    "    df_test_scaled = standard_scaler.transform(df_test)\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    labels_train = label_encoder.fit_transform(labels_train)\n",
    "    labels_test = label_encoder.transform(labels_test)\n",
    "\n",
    "    return df_train_scaled, df_test_scaled, labels_train, labels_test\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X =torch.tensor(X, dtype=torch.float)\n",
    "        self.y =torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    batch_size = 128\n",
    "    train_data = CustomDataset(X_train_scaled, y_train)\n",
    "    test_data = CustomDataset(X_test_scaled, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "# train_dataloader, test_dataloader = intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "enumerate() missing required argument 'iterable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Admin\\Learning\\UniProgrammingProjects\\SC4002_NLP\\Part1_model.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Admin/Learning/UniProgrammingProjects/SC4002_NLP/Part1_model.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(no_of_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Admin/Learning/UniProgrammingProjects/SC4002_NLP/Part1_model.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_loss, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Admin/Learning/UniProgrammingProjects/SC4002_NLP/Part1_model.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index, (sentence, tags) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Admin/Learning/UniProgrammingProjects/SC4002_NLP/Part1_model.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         pred \u001b[39m=\u001b[39m model(sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Admin/Learning/UniProgrammingProjects/SC4002_NLP/Part1_model.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(pred, tags)\n",
      "\u001b[1;31mTypeError\u001b[0m: enumerate() missing required argument 'iterable'"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, torch.unsqueeze(y,1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "        correct += torch.sum((pred > 0.5) == torch.unsqueeze(y,1))\n",
    "    \n",
    "        train_loss /= num_batches\n",
    "        correct = float(correct) / size\n",
    "    \n",
    "    return train_loss, correct\n",
    "    \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, torch.unsqueeze(y,1)).item()\n",
    "            correct += torch.sum((pred > 0.5) == torch.unsqueeze(y,1))\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct = float(correct) / size\n",
    " \n",
    "    return test_loss, correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_epochs = 3\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "data = ??\n",
    "\n",
    "\n",
    "for epoch in range(no_of_epochs):\n",
    "    train_loss, correct = 0, 0\n",
    "    for index, (sentence, tags) in enumerate(data):\n",
    "        pred = model(sentence)\n",
    "        loss = loss_fn(pred, tags)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += ?? \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        correct = float(correct) / size\n",
    "    \n",
    "    return train_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_google_news data type: <class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "word2vec_google_news = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(f\"word2vec_google_news data type: {type(word2vec_google_news)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_for_school = word2vec_google_news['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for school: [ 9.13085938e-02  2.81982422e-02  3.68652344e-02  2.18750000e-01\n",
      "  9.76562500e-02 -6.59179688e-02  1.56250000e-01 -4.68750000e-02\n",
      " -4.66918945e-03 -9.08203125e-02  2.49023438e-01  4.56542969e-02\n",
      "  6.64062500e-02 -9.86328125e-02 -1.05957031e-01  1.06445312e-01\n",
      " -2.00195312e-01  1.40991211e-02  1.36718750e-01 -8.78906250e-02\n",
      "  2.05078125e-01  1.70898438e-01  1.16577148e-02  1.71875000e-01\n",
      "  4.85229492e-03 -3.49609375e-01  2.27355957e-03  1.84570312e-01\n",
      "  1.19628906e-01 -2.51464844e-02  1.02050781e-01 -1.06933594e-01\n",
      "  4.00390625e-02  7.50732422e-03 -1.63085938e-01 -1.49414062e-01\n",
      "  2.28515625e-01 -5.02929688e-02  4.63867188e-02  1.25000000e-01\n",
      "  1.30615234e-02  1.27929688e-01  1.10351562e-01 -5.05371094e-02\n",
      "  3.44238281e-02  1.44531250e-01  1.71875000e-01 -2.78320312e-02\n",
      " -1.10839844e-01  2.29492188e-01 -1.17187500e-01 -1.22070312e-01\n",
      " -3.55529785e-03 -1.13769531e-01  2.11914062e-01 -2.08984375e-01\n",
      " -1.04492188e-01  4.39453125e-02 -1.23291016e-02 -1.59179688e-01\n",
      " -8.83789062e-02 -4.00390625e-02  1.06445312e-01 -5.07812500e-02\n",
      "  7.76367188e-02  1.46484375e-01 -1.72851562e-01  2.07031250e-01\n",
      "  2.20947266e-02  4.05273438e-02 -1.83593750e-01  1.21093750e-01\n",
      "  8.69140625e-02  3.68652344e-02 -1.11328125e-01  3.55468750e-01\n",
      "  2.87109375e-01  3.18908691e-03  1.72851562e-01  1.58203125e-01\n",
      "  4.98046875e-02 -1.66992188e-01 -1.50390625e-01  2.08007812e-01\n",
      "  5.51757812e-02  3.95507812e-02  3.71093750e-02  2.37304688e-01\n",
      "  1.66015625e-01 -2.83203125e-02 -1.81884766e-02 -1.89453125e-01\n",
      " -1.36718750e-02 -1.53320312e-01 -1.78222656e-02  9.42382812e-02\n",
      "  1.59179688e-01  6.34765625e-02 -6.25000000e-02  7.17773438e-02\n",
      " -4.78515625e-02 -1.33666992e-02 -1.38671875e-01  6.12792969e-02\n",
      " -2.17773438e-01 -8.34960938e-02  1.29394531e-02  5.56640625e-02\n",
      " -2.17285156e-02 -2.08007812e-01 -2.30468750e-01  1.69921875e-01\n",
      " -3.36914062e-02 -4.51660156e-02  1.65039062e-01 -2.51770020e-04\n",
      "  6.68945312e-02 -4.98046875e-02 -1.41601562e-01 -1.62109375e-01\n",
      " -1.12304688e-01 -1.51977539e-02  2.47802734e-02 -2.17285156e-02\n",
      " -5.95703125e-02 -4.51660156e-02 -8.25195312e-02 -1.35742188e-01\n",
      "  3.06701660e-03  8.20312500e-02  9.08203125e-02 -3.55468750e-01\n",
      " -1.94335938e-01 -1.02050781e-01 -1.32812500e-01 -2.99072266e-02\n",
      "  2.72216797e-02 -1.04308128e-05 -2.78320312e-02  7.03125000e-02\n",
      " -3.55468750e-01 -3.55468750e-01  1.29882812e-01  4.57763672e-03\n",
      " -2.61230469e-02  2.14843750e-01 -9.66796875e-02 -9.52148438e-02\n",
      " -2.22167969e-02  4.74929810e-04  4.04296875e-01 -1.77734375e-01\n",
      " -9.03320312e-02  1.49414062e-01 -2.06054688e-01 -1.28906250e-01\n",
      "  2.55859375e-01 -2.39257812e-01  1.07421875e-01 -7.27539062e-02\n",
      " -2.27539062e-01  1.13769531e-01  2.04101562e-01  1.95312500e-01\n",
      " -7.76367188e-02  1.15356445e-02 -3.07617188e-02 -2.09960938e-01\n",
      "  6.12792969e-02  5.27343750e-02 -2.17773438e-01  3.93066406e-02\n",
      "  1.58691406e-02 -2.63671875e-01  1.68457031e-02  6.54296875e-02\n",
      "  1.72119141e-02  1.22558594e-01  1.04370117e-02  2.47070312e-01\n",
      " -3.17382812e-03  5.81054688e-02  1.70898438e-02  4.88281250e-02\n",
      "  8.49609375e-02  1.28173828e-02 -8.59375000e-02 -6.29882812e-02\n",
      "  2.25585938e-01  9.22851562e-02  7.27539062e-02 -9.03320312e-02\n",
      " -2.17285156e-02  1.78710938e-01 -2.67578125e-01 -8.20312500e-02\n",
      "  1.15234375e-01 -1.87500000e-01 -7.35473633e-03 -1.95312500e-02\n",
      " -1.05957031e-01 -2.19726562e-02  2.13867188e-01 -2.83203125e-01\n",
      " -6.73828125e-02 -2.92968750e-01  1.74560547e-02 -2.53906250e-01\n",
      " -5.00488281e-02 -1.42669678e-03  8.93554688e-02 -1.51367188e-01\n",
      "  1.77734375e-01 -3.57421875e-01 -3.26171875e-01  3.24218750e-01\n",
      " -1.01562500e-01  5.95703125e-02 -1.35742188e-01 -1.43554688e-01\n",
      " -1.24511719e-01 -9.22851562e-02  2.50244141e-02  1.46484375e-01\n",
      "  1.23046875e-01  2.73437500e-02 -2.57812500e-01  1.13281250e-01\n",
      "  7.66601562e-02 -3.68652344e-02  2.08007812e-01  7.91015625e-02\n",
      "  4.98046875e-02  1.61132812e-01  2.12402344e-02  8.39843750e-02\n",
      "  9.09423828e-03 -1.93359375e-01  1.62353516e-02 -2.95410156e-02\n",
      "  7.03125000e-02 -5.29785156e-02  2.30468750e-01 -8.74023438e-02\n",
      " -2.73437500e-01 -1.16210938e-01 -1.64062500e-01 -6.59179688e-02\n",
      "  1.63574219e-02 -6.13403320e-03  3.19824219e-02  9.76562500e-02\n",
      "  1.38671875e-01  1.42578125e-01 -8.74023438e-02 -2.67578125e-01\n",
      "  1.15722656e-01 -2.60009766e-02  1.92871094e-02  2.51953125e-01\n",
      "  4.58984375e-02  1.39648438e-01 -2.47070312e-01 -9.22851562e-02\n",
      "  9.57031250e-02 -5.61523438e-02  3.63769531e-02  6.64062500e-02\n",
      " -4.66308594e-02  5.98144531e-02  1.01562500e-01 -4.68750000e-02\n",
      "  9.08203125e-02  2.08984375e-01 -7.81250000e-02 -2.51953125e-01\n",
      "  3.06640625e-01 -3.14453125e-01 -6.07910156e-02 -3.10546875e-01\n",
      " -1.17187500e-01 -5.05371094e-02 -1.36718750e-01  2.51953125e-01\n",
      " -1.91406250e-01 -4.88281250e-04 -9.32617188e-02  1.96289062e-01\n",
      "  1.51367188e-01  2.98828125e-01 -1.07910156e-01 -2.39257812e-02\n",
      " -1.47460938e-01 -6.68945312e-02  1.04980469e-01 -5.73730469e-02\n",
      " -1.05957031e-01 -1.25122070e-02  9.08203125e-02  1.20117188e-01]\n",
      "type: <class 'numpy.ndarray'>\n",
      "shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector for school: {word_embedding_for_school}\", end=\"\\n\")\n",
    "print(f\"type: {type(word_embedding_for_school)}\", end=\"\\n\")\n",
    "print(f\"shape: {word_embedding_for_school.shape}\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cz4045_group_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
