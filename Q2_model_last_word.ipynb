{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roy_l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T06:06:49.329536800Z",
     "start_time": "2023-11-09T06:06:48.994363800Z"
    }
   },
   "id": "16fcb8704f34334f"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e4d9acf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:45.186156600Z",
     "start_time": "2023-11-09T06:06:49.041726500Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "446beddd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:45.308998500Z",
     "start_time": "2023-11-09T06:07:45.209711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47079fad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:45.308998500Z",
     "start_time": "2023-11-09T06:07:45.224722500Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    setences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "    return setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e894b3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.185133500Z",
     "start_time": "2023-11-09T06:07:45.244055300Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3de5ed02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.285079100Z",
     "start_time": "2023-11-09T06:07:46.188133200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.300649Z",
     "start_time": "2023-11-09T06:07:46.293640500Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.341177500Z",
     "start_time": "2023-11-09T06:07:46.301654100Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.005) \n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 150)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(150)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(150, number_of_tags)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len.\n",
    "            lengths: (list) contains the original lengths of the sequences in the batch.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size, seq_len, embedding_dim = padded_output.size()\n",
    "        s = self.batch_norm1(padded_output.view(-1, embedding_dim))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        s = s.view(batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        s = s[:, -1, :] # last word\n",
    "        s = self.fc1(s)\n",
    "        s = self.batch_norm2(s)\n",
    "        s = self.relu(s)\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc2(s)\n",
    "        out = self.batch_norm3(s)\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.342181400Z",
     "start_time": "2023-11-09T06:07:46.324283Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e07330c216283eda",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.342181400Z",
     "start_time": "2023-11-09T06:07:46.331655800Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84b8f1d154a50053",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:07:46.363234400Z",
     "start_time": "2023-11-09T06:07:46.346177500Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "    return loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    loss_avg = RunningAverage()\n",
    "    accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        accuracy_avg.update(accuracy_val)\n",
    "        \n",
    "    print(f\"{loss_avg()=}\")\n",
    "    print(f\"{accuracy_avg()=}\")\n",
    "    return loss_avg()\n",
    "    \n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        a = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "\n",
    "        if early_stopper.early_stop(a):             \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:09:04.950853700Z",
     "start_time": "2023-11-09T06:07:46.355234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 62.60it/s, loss=1.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.4268899281819662\n",
      "accuracy_avg()=0.3854166666666667\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 66.38it/s, loss=1.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.2723707199096679\n",
      "accuracy_avg()=0.46875\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 64.17it/s, loss=1.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.909099531173706\n",
      "accuracy_avg()=0.6375\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 66.67it/s, loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.8269427021344503\n",
      "accuracy_avg()=0.6875\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 63.82it/s, loss=0.707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7191881696383159\n",
      "accuracy_avg()=0.75\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 62.47it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6253210604190826\n",
      "accuracy_avg()=0.7833333333333333\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 63.98it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6017907758553823\n",
      "accuracy_avg()=0.79375\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 64.42it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5862593760093053\n",
      "accuracy_avg()=0.7979166666666667\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 62.77it/s, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5771923472483953\n",
      "accuracy_avg()=0.8083333333333333\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 63.90it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.590949281056722\n",
      "accuracy_avg()=0.8041666666666667\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 61.37it/s, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.543566033244133\n",
      "accuracy_avg()=0.8291666666666667\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.20it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6427935183048248\n",
      "accuracy_avg()=0.80625\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.22it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4968042512734731\n",
      "accuracy_avg()=0.8583333333333333\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.36it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.49316703577836357\n",
      "accuracy_avg()=0.8625\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.77it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.542041751742363\n",
      "accuracy_avg()=0.8416666666666667\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.29it/s, loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.496762615442276\n",
      "accuracy_avg()=0.8458333333333333\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.56it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.47940549353758494\n",
      "accuracy_avg()=0.8604166666666667\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.63it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4851339300473531\n",
      "accuracy_avg()=0.8625\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.37it/s, loss=0.240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5071260412534078\n",
      "accuracy_avg()=0.85\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 74.50it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4880554676055908\n",
      "accuracy_avg()=0.8583333333333333\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.49it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.4783234844605128\n",
      "accuracy_avg()=0.8625\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 74.07it/s, loss=0.180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.49459461867809296\n",
      "accuracy_avg()=0.8625\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 67.35it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.50946044921875\n",
      "accuracy_avg()=0.85625\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.69it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5186278750499089\n",
      "accuracy_avg()=0.8583333333333333\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.72it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6030013859272003\n",
      "accuracy_avg()=0.8479166666666667\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 70.80it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5593254804611206\n",
      "accuracy_avg()=0.8520833333333333\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 75.71it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5239010989665985\n",
      "accuracy_avg()=0.8625\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.06it/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5843065778414408\n",
      "accuracy_avg()=0.8333333333333334\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.40it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5212418099244436\n",
      "accuracy_avg()=0.85625\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 71.69it/s, loss=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5333161969979604\n",
      "accuracy_avg()=0.8708333333333333\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 70.84it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5801074494918187\n",
      "accuracy_avg()=0.85625\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.54it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5824067940314611\n",
      "accuracy_avg()=0.85625\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.51it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.5739973167578379\n",
      "accuracy_avg()=0.8520833333333333\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 73.44it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6448029965162277\n",
      "accuracy_avg()=0.8541666666666666\n"
     ]
    }
   ],
   "source": [
    "model = Net(embedding_weights, 300, 5, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# if (os.path.isfile(\"model_weights_last_word.pth\")):\n",
    "#     model.load_state_dict(torch.load('model_weights_last_word.pth'))\n",
    "# else:\n",
    "#     start_time = time.time()\n",
    "#     train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 300, 32, optimizer, loss_fn)\n",
    "#     torch.save(model.state_dict(), 'model_weights_last_word.pth')\n",
    "#     end_time = time.time()\n",
    "#     total_time = end_time - start_time\n",
    "#     print(\"total training time taken:\", total_time)\n",
    "\n",
    "train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 100, 32, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec728a-fcae-4748-9a93-0f3ccbf76c74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9be8268f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:09:05.029091200Z",
     "start_time": "2023-11-09T06:09:04.957854800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.854\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T06:17:11.440904900Z",
     "start_time": "2023-11-09T06:17:11.383013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = What is a squirrel ?, label = 0\n",
      "sentence = Is Singapore located in Southeast Asia ?, label = 0\n",
      "sentence = Is Singapore in China ?, label = 0\n",
      "sentence = Name 11 famous martyrs ., label = 4\n",
      "sentence = What ISPs exist in the Caribbean ?, label = 1\n",
      "sentence = How many cars are manufactured every day ?, label = 4\n"
     ]
    }
   ],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel ?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia ?\")\n",
    "print_sentence_label(\"Is Singapore in China ?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
