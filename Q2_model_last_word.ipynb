{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16fcb8704f34334f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:30:29.733071500Z",
     "start_time": "2023-11-09T05:30:29.367452900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roy_l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e4d9acf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:31.276232600Z",
     "start_time": "2023-11-09T05:30:29.417451100Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "446beddd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:31.302560800Z",
     "start_time": "2023-11-09T05:31:31.279223100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e762f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47079fad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:31.370148600Z",
     "start_time": "2023-11-09T05:31:31.318558Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_text):\n",
    "    tokenized = []\n",
    "    for sentence in list_of_text:\n",
    "        tokenized.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized\n",
    "\n",
    "def format_label(label):\n",
    "    return torch.unsqueeze(torch.tensor(label.to_list()), axis=1).tolist()\n",
    "\n",
    "def indexify(data):\n",
    "    setences = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "    return setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e894b3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:32.251242400Z",
     "start_time": "2023-11-09T05:31:31.332116100Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified csv files are derived from running Q2_preprocessing.ipynb\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_training_data.csv\", sep=\",\") \n",
    "test_data = pd.read_csv(filepath_or_buffer=\"TREC_dataset/modified_test_data.csv\", sep=\",\")\n",
    "\n",
    "X = training_data[\"text\"]\n",
    "y = training_data[\"label-coarse\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=500) # get 500 samples for validation set\n",
    "\n",
    "X_test = test_data[\"text\"]\n",
    "y_test = test_data[\"label-coarse\"]\n",
    "\n",
    "X_train_lst = X_train.to_list()\n",
    "X_val_lst = X_val.to_list()\n",
    "X_test_lst = X_test.to_list()\n",
    "\n",
    "X_train_tokenized = tokenize_sentences(X_train_lst)\n",
    "X_val_tokenized = tokenize_sentences(X_val_lst)\n",
    "X_test_tokenized = tokenize_sentences(X_test_lst)\n",
    "\n",
    "no_of_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3de5ed02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:32.366114600Z",
     "start_time": "2023-11-09T05:31:32.254242900Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized_indexified = indexify(X_train_tokenized)\n",
    "X_val_tokenized_indexified = indexify(X_val_tokenized)\n",
    "X_test_tokenized_indexified = indexify(X_test_tokenized)\n",
    "\n",
    "y_train_formatted = format_label(y_train)\n",
    "y_val_formatted = format_label(y_val)\n",
    "y_test_formatted = format_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:32.391637Z",
     "start_time": "2023-11-09T05:31:32.369112600Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = np.array(batch_tags).squeeze()\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:31:32.414733600Z",
     "start_time": "2023-11-09T05:31:32.387640100Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim and pad_index\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len.\n",
    "            lengths: (list) contains the original lengths of the sequences in the batch.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        \"\"\"\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # pack the sequences before feeding them to the LSTM\n",
    "        packed_input = pack_padded_sequence(s, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "\n",
    "        # unpack the sequences after passing through the LSTM\n",
    "        padded_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        s = padded_output[:, -1, :] # last word\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        out = self.fc(s)\n",
    "\n",
    "        # apply log softmax on each token's output\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:47:47.722864Z",
     "start_time": "2023-11-09T05:47:47.662027700Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    loss = F.cross_entropy(outputs, labels.squeeze())\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e07330c216283eda",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:47:51.715987900Z",
     "start_time": "2023-11-09T05:47:51.694492800Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84b8f1d154a50053",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:47:51.739575300Z",
     "start_time": "2023-11-09T05:47:51.706956700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "        train_batch = train_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        seq_lengths = torch.LongTensor(list(map(len, train_batch)))\n",
    "        output_batch = model(train_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "    return loss_avg()\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    loss_avg = RunningAverage()\n",
    "    accuracy_avg = RunningAverage()\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # compute model output\n",
    "        seq_lengths = torch.LongTensor(list(map(len, data_batch)))\n",
    "        output_batch = model(data_batch, seq_lengths)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        loss_avg.update(loss.item())\n",
    "        accuracy_val = accuracy(output_batch, labels_batch)\n",
    "        accuracy_avg.update(accuracy_val)\n",
    "        \n",
    "    print(f\"{loss_avg()=}\")\n",
    "    print(f\"{accuracy_avg()=}\")\n",
    "    return loss_avg()\n",
    "    \n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn\n",
    "):\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        a = evaluate(model, loss_fn, val_data_iterator, num_steps)\n",
    "\n",
    "        if early_stopper.early_stop(a):             \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:49:29.352241500Z",
     "start_time": "2023-11-09T05:48:17.024954800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 79.06it/s, loss=1.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.393842355410258\n",
      "accuracy_avg()=0.4\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 86.42it/s, loss=1.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.3251364628473918\n",
      "accuracy_avg()=0.46458333333333335\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 85.75it/s, loss=1.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.199426563580831\n",
      "accuracy_avg()=0.475\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.81it/s, loss=1.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=1.0519217014312745\n",
      "accuracy_avg()=0.5604166666666667\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 82.80it/s, loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.9446425795555115\n",
      "accuracy_avg()=0.6\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.42it/s, loss=0.820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.896989893913269\n",
      "accuracy_avg()=0.6270833333333333\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 85.22it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.8514403283596039\n",
      "accuracy_avg()=0.675\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.52it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7579094688097636\n",
      "accuracy_avg()=0.7770833333333333\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 72.30it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7276986837387085\n",
      "accuracy_avg()=0.7833333333333333\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 62.75it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6986889173587163\n",
      "accuracy_avg()=0.8020833333333334\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 65.45it/s, loss=0.450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7218493858973185\n",
      "accuracy_avg()=0.8\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 75.60it/s, loss=0.410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6570332954327266\n",
      "accuracy_avg()=0.8125\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 81.01it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6624687383572261\n",
      "accuracy_avg()=0.8104166666666667\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 81.61it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.673242234190305\n",
      "accuracy_avg()=0.80625\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 76.73it/s, loss=0.330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7053775270779927\n",
      "accuracy_avg()=0.79375\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 80.33it/s, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7059938381115596\n",
      "accuracy_avg()=0.80625\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.42it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.727364965279897\n",
      "accuracy_avg()=0.7958333333333333\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 75.53it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7083529442548752\n",
      "accuracy_avg()=0.80625\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 81.65it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6848751078049342\n",
      "accuracy_avg()=0.8208333333333333\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 82.80it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6822229613860448\n",
      "accuracy_avg()=0.8229166666666666\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 76.58it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6785704473654429\n",
      "accuracy_avg()=0.8270833333333333\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 81.31it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6864738434553146\n",
      "accuracy_avg()=0.8270833333333333\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.43it/s, loss=0.220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7038647472858429\n",
      "accuracy_avg()=0.8229166666666666\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 70.90it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6906436632076899\n",
      "accuracy_avg()=0.825\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.06it/s, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7327058454354604\n",
      "accuracy_avg()=0.81875\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.48it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7572577834129334\n",
      "accuracy_avg()=0.8104166666666667\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 76.62it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6850665678580602\n",
      "accuracy_avg()=0.8270833333333333\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 84.62it/s, loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6900741537412007\n",
      "accuracy_avg()=0.8270833333333333\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.97it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.6869787136713664\n",
      "accuracy_avg()=0.8291666666666667\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:02<00:00, 76.08it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7348351597785949\n",
      "accuracy_avg()=0.8291666666666667\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.51it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7090250730514527\n",
      "accuracy_avg()=0.8395833333333333\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.74it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7279250462849934\n",
      "accuracy_avg()=0.8270833333333333\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 77.31it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7577039579550425\n",
      "accuracy_avg()=0.8416666666666667\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 82.97it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7939366320768992\n",
      "accuracy_avg()=0.8291666666666667\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 83.51it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.7927799900372823\n",
      "accuracy_avg()=0.83125\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 80.84it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_avg()=0.8130448242028554\n",
      "accuracy_avg()=0.825\n"
     ]
    }
   ],
   "source": [
    "model = Net(embedding_weights, 300, 5, no_of_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# if (os.path.isfile(\"model_weights_last_word.pth\")):\n",
    "#     model.load_state_dict(torch.load('model_weights_last_word.pth'))\n",
    "# else:\n",
    "#     start_time = time.time()\n",
    "#     train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 300, 32, optimizer, loss_fn)\n",
    "#     torch.save(model.state_dict(), 'model_weights_last_word.pth')\n",
    "#     end_time = time.time()\n",
    "#     total_time = end_time - start_time\n",
    "#     print(\"total training time taken:\", total_time)\n",
    "\n",
    "train_and_evaluate(model, X_train_tokenized_indexified , y_train_formatted , X_val_tokenized_indexified  , y_val_formatted, 100, 32, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec728a-fcae-4748-9a93-0f3ccbf76c74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9be8268f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:42:13.154994Z",
     "start_time": "2023-11-09T05:42:13.058223600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy=0.844\n"
     ]
    }
   ],
   "source": [
    "# Simple check with test dataset\n",
    "model.eval()\n",
    "test_data_iterator = data_iterator(X_test_tokenized_indexified, y_test_formatted, len(X_test_tokenized_indexified), len(X_test_tokenized_indexified), shuffle=False)\n",
    "test_batch, labels_batch, test_sentences = next(test_data_iterator)\n",
    "\n",
    "seq_lengths = torch.LongTensor(list(map(len, test_batch)))\n",
    "output_batch = model(test_batch.to(device),seq_lengths)\n",
    "final_test_accuracy = accuracy(output_batch, labels_batch.to(device))\n",
    "print(f\"{final_test_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60d0b775-ae76-40bb-b063-11b922bd78bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T05:42:13.155965600Z",
     "start_time": "2023-11-09T05:42:13.097090800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = What is a squirrel?, label = 3\n",
      "sentence = Is Singapore located in Southeast Asia?, label = 3\n",
      "sentence = Is Singapore in China?, label = 0\n",
      "sentence = Name 11 famous martyrs ., label = 3\n",
      "sentence = What ISPs exist in the Caribbean ?, label = 0\n",
      "sentence = How many cars are manufactured every day?, label = 4\n"
     ]
    }
   ],
   "source": [
    "def print_sentence_label(sentence: str) -> int:\n",
    "    model.eval()\n",
    "    sentence_tokenized = word_tokenize(sentence.lower())\n",
    "    sentence_as_id = [\n",
    "        vocab[token] if token in vocab\n",
    "        else vocab['UNK']\n",
    "        for token in sentence_tokenized\n",
    "    ]\n",
    "    seq_lengths = torch.LongTensor([len(sentence_as_id)])\n",
    "    input = torch.tensor(sentence_as_id).unsqueeze(0).to(device)\n",
    "    output = model(input, seq_lengths).to(device)\n",
    "    label = np.argmax(output.detach().cpu().numpy())\n",
    "    print(f\"sentence = {sentence}, label = {label}\")\n",
    "\n",
    "# Checking results\n",
    "print_sentence_label(\"What is a squirrel?\")\n",
    "print_sentence_label(\"Is Singapore located in Southeast Asia?\")\n",
    "print_sentence_label(\"Is Singapore in China?\")\n",
    "print_sentence_label(\"Name 11 famous martyrs .\")\n",
    "print_sentence_label(\"What ISPs exist in the Caribbean ?\")\n",
    "print_sentence_label(\"How many cars are manufactured every day?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
