{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347520df11b3027e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcb8704f34334f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.923216Z",
     "start_time": "2023-10-16T14:37:13.814857Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4d9acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\envs\\cz4045_group_assignment\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "word2vec_goog1e_news: gensim.models.keyedvectors.KeyedVectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec_goog1e_news.add_vector(\"<pad>\", np.zeros(300))\n",
    "pad_index = word2vec_goog1e_news.key_to_index[\"<pad>\"]\n",
    "embedding_weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "vocab = word2vec_goog1e_news.key_to_index\n",
    "# UNK is already inside word2vec_google_news\n",
    "# weights = torch.FloatTensor(word2vec_goog1e_news.vectors)\n",
    "# embedding = nn.Embedding.from_pretrained(weights)\n",
    "# WORD = 'UNK'\n",
    "# WORDID = word2vec_goog1e_news.key_to_index[WORD]\n",
    "# print(f\"WORDID: {WORDID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f176639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    tag = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:  # Empty line indicates the end of a sentence\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                if word == '-DOCSTART-':\n",
    "                    continue\n",
    "                ner_tag = parts[-1]\n",
    "                sentence.append(word)\n",
    "                tag.append(ner_tag)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag)\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e894b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_tags = read_conll_file('CoNLL2003_dataset/eng.train')\n",
    "val_data, val_tags = read_conll_file('CoNLL2003_dataset/eng.testa')\n",
    "test_data, test_tags = read_conll_file('CoNLL2003_dataset/eng.testb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_unique_words(allSentences):\n",
    "    uniqueWords = [] \n",
    "    flatten_sentences = list(chain(*allSentences))\n",
    "    for i in flatten_sentences:\n",
    "        if not i in uniqueWords:\n",
    "            uniqueWords.append(i);\n",
    "    uniqueWords.append('<pad>')\n",
    "    uniqueWords.append('UNK')\n",
    "    return uniqueWords\n",
    "\n",
    "def get_unique_tags(allTags):\n",
    "    uniqueWords = []\n",
    "    flatten_sentences = list(chain(*allTags))\n",
    "    for i in flatten_sentences:\n",
    "        if not i in uniqueWords:\n",
    "            uniqueWords.append(i);\n",
    "    return uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = get_unique_words(train_data+val_data) # test_data not included\n",
    "unique_tags = get_unique_tags(train_tags+val_tags+test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6c1bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-ORG': 0,\n",
       " 'O': 1,\n",
       " 'I-MISC': 2,\n",
       " 'I-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-LOC': 5,\n",
       " 'B-MISC': 6,\n",
       " 'B-ORG': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map = {}\n",
    "for index, tag in enumerate(unique_tags):\n",
    "    tag_map[tag] = index\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb4a69d3a6f4b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:13.991538Z",
     "start_time": "2023-10-16T14:37:13.847742Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indexify(data, tag):\n",
    "    setences = []\n",
    "    labels = []\n",
    "    for sentence in data:\n",
    "        s = [vocab[token] if token in vocab\n",
    "            else vocab['UNK']\n",
    "            for token in sentence]\n",
    "        setences.append(s)\n",
    "\n",
    "    for sentence in tag:\n",
    "        l = [tag_map[label] for label in sentence]\n",
    "        labels.append(l)\n",
    "\n",
    "    return setences, labels\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "train_sentences, train_labels = indexify(train_data, train_tags)\n",
    "val_sentences, val_labels = indexify(val_data, val_tags)\n",
    "test_sentences, test_labels = indexify(test_data, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"train_sentences: \\n {train_sentences}\")\n",
    "# print(f\"train_labels: \\n {train_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5800cc7f3fc93583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.011887Z",
     "start_time": "2023-10-16T14:37:13.882822Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iterator(sentences, labels, total_size: int, batch_size: int, shuffle: bool=False):\n",
    "    # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "    order = list(range(total_size))\n",
    "    if shuffle:\n",
    "        random.seed(230)\n",
    "        random.shuffle(order)\n",
    "\n",
    "    # one pass over data\n",
    "    for i in range((total_size+1)//batch_size):\n",
    "        # fetch sentences and tags\n",
    "        batch_sentences = [sentences[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "        batch_tags = [labels[idx] for idx in order[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "        # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "        # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "        batch_data = vocab['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "        # copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "        # since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        # if params.cuda:\n",
    "        #     batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        yield batch_data, batch_labels, batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9aa7ca1ef91b423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.012307Z",
     "start_time": "2023-10-16T14:37:13.894126Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
    "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
    "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
    "    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n",
    "\n",
    "    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n",
    "    you can go about defining your own network.\n",
    "\n",
    "    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_weights, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        \"\"\"\n",
    "        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n",
    "        required are:\n",
    "\n",
    "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "\n",
    "        Args:\n",
    "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # the embedding takes as input the vocab_size and the embedding_dim\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, padding_idx=pad_index)\n",
    "\n",
    "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
    "        # for more details on how to use it, check out the documentation\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # the fully connected layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This function defines how we use the components of our network to operate on an input batch.\n",
    "\n",
    "        Args:\n",
    "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "\n",
    "        Returns:\n",
    "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        s = self.embedding(s)\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "        s, _ = self.lstm(s)\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "        s = s.view(-1, s.shape[2])\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba90bdc3ae13684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.012907Z",
     "start_time": "2023-10-16T14:37:13.899592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07330c216283eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.013393Z",
     "start_time": "2023-10-16T14:37:13.913339Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b8f1d154a50053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014581Z",
     "start_time": "2023-10-16T14:37:13.927378Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % 10 == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1feee96f562ef39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014871Z",
     "start_time": "2023-10-16T14:37:13.939867Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, _ = next(data_iterator)\n",
    "\n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798027bbaa11f757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.014985Z",
     "start_time": "2023-10-16T14:37:13.951561Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        model,\n",
    "        train_sentences,\n",
    "        train_labels,\n",
    "        val_sentences,\n",
    "        val_labels,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        metrics\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run one epoch\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (len(train_sentences) + 1) // batch_size\n",
    "        train_data_iterator = data_iterator(\n",
    "            train_sentences, train_labels, len(train_sentences), batch_size, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator,\n",
    "              metrics, num_steps)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (len(val_sentences) + 1) // batch_size\n",
    "        val_data_iterator = data_iterator(\n",
    "            val_sentences, val_labels, len(val_sentences), batch_size, shuffle=False)\n",
    "        val_metrics = evaluate(\n",
    "            model, loss_fn, val_data_iterator, metrics, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a981f205b1133b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.015072Z",
     "start_time": "2023-10-16T14:37:13.960431Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca478d1db5c66ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:37:14.520501Z",
     "start_time": "2023-10-16T14:37:13.975258Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 136.48it/s, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.945 ; loss: 0.182\n",
      "- Eval metrics : accuracy: 0.963 ; loss: 0.124\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.17it/s, loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.972 ; loss: 0.099\n",
      "- Eval metrics : accuracy: 0.967 ; loss: 0.108\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 140.03it/s, loss=0.081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.977 ; loss: 0.079\n",
      "- Eval metrics : accuracy: 0.970 ; loss: 0.103\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.71it/s, loss=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.982 ; loss: 0.061\n",
      "- Eval metrics : accuracy: 0.971 ; loss: 0.104\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.16it/s, loss=0.050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.987 ; loss: 0.046\n",
      "- Eval metrics : accuracy: 0.971 ; loss: 0.109\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.57it/s, loss=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.990 ; loss: 0.037\n",
      "- Eval metrics : accuracy: 0.971 ; loss: 0.120\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.74it/s, loss=0.032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.992 ; loss: 0.029\n",
      "- Eval metrics : accuracy: 0.970 ; loss: 0.130\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.88it/s, loss=0.027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.993 ; loss: 0.025\n",
      "- Eval metrics : accuracy: 0.970 ; loss: 0.132\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.57it/s, loss=0.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.994 ; loss: 0.022\n",
      "- Eval metrics : accuracy: 0.969 ; loss: 0.135\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2808/2808 [00:20<00:00, 139.57it/s, loss=0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.994 ; loss: 0.020\n",
      "- Eval metrics : accuracy: 0.969 ; loss: 0.145\n"
     ]
    }
   ],
   "source": [
    "# manually change vocab size (unique no. of words) and change label size (unique no. of labels) for now\n",
    "model = Net(embedding_weights, 300, 300, len(tag_map))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_and_evaluate(model, train_sentences, train_labels, val_sentences, val_labels, 10, 5, optimizer, loss_fn, metrics)\n",
    "\n",
    "if (os.path.isfile(\"model_weights.pth\")):\n",
    "    model.load_state_dict(torch.load('model_weights.pth'))\n",
    "else:\n",
    "    torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453\n",
      "tensor([[-1.7358e+01, -1.3741e+01, -1.6243e+01, -1.3113e-06, -1.5743e+01,\n",
      "         -2.7139e+01, -2.2746e+01, -2.8044e+01],\n",
      "        [-2.2845e+01, -4.1723e-06, -1.9414e+01, -1.2396e+01, -2.1665e+01,\n",
      "         -2.3541e+01, -2.1114e+01, -2.6349e+01],\n",
      "        [-2.1759e+01, -5.9605e-07, -1.7011e+01, -1.4610e+01, -1.6003e+01,\n",
      "         -1.8679e+01, -1.7537e+01, -2.0842e+01],\n",
      "        [-2.2305e+01,  0.0000e+00, -2.5897e+01, -2.0426e+01, -2.1745e+01,\n",
      "         -2.3906e+01, -2.6087e+01, -2.6875e+01],\n",
      "        [-1.8704e+01,  0.0000e+00, -2.5221e+01, -2.0847e+01, -1.8749e+01,\n",
      "         -2.4449e+01, -2.8032e+01, -2.7926e+01],\n",
      "        [-2.0037e+01, -5.9245e-05, -1.9885e+01, -9.7332e+00, -1.7169e+01,\n",
      "         -2.2564e+01, -2.4068e+01, -2.4535e+01],\n",
      "        [-2.0029e+01, -2.3842e-07, -1.8565e+01, -1.5409e+01, -2.1263e+01,\n",
      "         -2.1737e+01, -2.2686e+01, -2.3695e+01],\n",
      "        [-2.2727e+01, -5.1378e-05, -9.9078e+00, -1.3487e+01, -1.7079e+01,\n",
      "         -1.6679e+01, -1.6464e+01, -1.9000e+01],\n",
      "        [-1.6054e+01, -3.9339e-06, -1.3560e+01, -1.4836e+01, -1.6914e+01,\n",
      "         -1.3548e+01, -1.4081e+01, -1.5697e+01],\n",
      "        [-2.3059e+01,  0.0000e+00, -2.0703e+01, -2.7625e+01, -2.1612e+01,\n",
      "         -2.7876e+01, -2.8177e+01, -3.1263e+01],\n",
      "        [-9.7909e+00, -2.9305e-02, -3.5514e+00, -1.0218e+01, -9.8534e+00,\n",
      "         -1.6113e+01, -9.9088e+00, -1.8881e+01],\n",
      "        [-8.9138e+00, -4.6433e-04, -8.9670e+00, -1.4603e+01, -1.2880e+01,\n",
      "         -1.2988e+01, -8.5329e+00, -1.7950e+01],\n",
      "        [-1.1228e+01, -5.5073e-05, -1.9197e+01, -2.0857e+01, -1.0084e+01,\n",
      "         -1.8773e+01, -1.7634e+01, -2.3469e+01],\n",
      "        [-1.7226e+01, -2.8010e-04, -1.8082e+01, -1.8253e+01, -8.1805e+00,\n",
      "         -1.7759e+01, -2.1361e+01, -2.4223e+01],\n",
      "        [-1.4345e+01, -6.5389e-04, -1.1798e+01, -1.1477e+01, -7.3702e+00,\n",
      "         -1.2178e+01, -1.4788e+01, -1.8827e+01],\n",
      "        [-5.6057e+00, -1.6396e-01, -3.0181e+00, -6.4427e+00, -2.3328e+00,\n",
      "         -1.4915e+01, -1.0342e+01, -1.8905e+01],\n",
      "        [-5.4870e+00, -1.1324e+01, -7.3734e+00, -1.1045e+01, -4.8152e-03,\n",
      "         -1.4428e+01, -1.1891e+01, -2.1606e+01],\n",
      "        [-1.7244e+01, -1.1921e-07, -1.6559e+01, -1.9878e+01, -1.6664e+01,\n",
      "         -1.7382e+01, -1.6643e+01, -2.2471e+01],\n",
      "        [-2.0673e+01,  0.0000e+00, -1.8261e+01, -2.3583e+01, -1.8425e+01,\n",
      "         -2.4783e+01, -2.5914e+01, -3.0767e+01],\n",
      "        [-1.8050e+01, -5.9605e-07, -1.7234e+01, -1.4574e+01, -1.6867e+01,\n",
      "         -1.5620e+01, -1.7242e+01, -1.8326e+01],\n",
      "        [-2.1681e+01,  0.0000e+00, -2.3551e+01, -1.9405e+01, -2.2007e+01,\n",
      "         -2.5855e+01, -2.7213e+01, -2.9788e+01],\n",
      "        [-1.2355e+01, -5.0068e-06, -1.5613e+01, -1.4985e+01, -1.5252e+01,\n",
      "         -2.5812e+01, -2.2969e+01, -2.8616e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([22, 8])\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))\n",
    "test_data_iterator = data_iterator(test_sentences, test_labels, len(test_sentences), 1, shuffle=True)\n",
    "test_batch, labels_batch, abc = next(test_data_iterator)\n",
    "model_output = model(test_batch)\n",
    "print(model_output)\n",
    "print(model_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff42671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output \n",
      " tensor([[-1.7358e+01, -1.3741e+01, -1.6243e+01, -1.3113e-06, -1.5743e+01,\n",
      "         -2.7139e+01, -2.2746e+01, -2.8044e+01],\n",
      "        [-2.2845e+01, -4.1723e-06, -1.9414e+01, -1.2396e+01, -2.1665e+01,\n",
      "         -2.3541e+01, -2.1114e+01, -2.6349e+01],\n",
      "        [-2.1759e+01, -5.9605e-07, -1.7011e+01, -1.4610e+01, -1.6003e+01,\n",
      "         -1.8679e+01, -1.7537e+01, -2.0842e+01],\n",
      "        [-2.2305e+01,  0.0000e+00, -2.5897e+01, -2.0426e+01, -2.1745e+01,\n",
      "         -2.3906e+01, -2.6087e+01, -2.6875e+01],\n",
      "        [-1.8704e+01,  0.0000e+00, -2.5221e+01, -2.0847e+01, -1.8749e+01,\n",
      "         -2.4449e+01, -2.8032e+01, -2.7926e+01],\n",
      "        [-2.0037e+01, -5.9245e-05, -1.9885e+01, -9.7332e+00, -1.7169e+01,\n",
      "         -2.2564e+01, -2.4068e+01, -2.4535e+01],\n",
      "        [-2.0029e+01, -2.3842e-07, -1.8565e+01, -1.5409e+01, -2.1263e+01,\n",
      "         -2.1737e+01, -2.2686e+01, -2.3695e+01],\n",
      "        [-2.2727e+01, -5.1378e-05, -9.9078e+00, -1.3487e+01, -1.7079e+01,\n",
      "         -1.6679e+01, -1.6464e+01, -1.9000e+01],\n",
      "        [-1.6054e+01, -3.9339e-06, -1.3560e+01, -1.4836e+01, -1.6914e+01,\n",
      "         -1.3548e+01, -1.4081e+01, -1.5697e+01],\n",
      "        [-2.3059e+01,  0.0000e+00, -2.0703e+01, -2.7625e+01, -2.1612e+01,\n",
      "         -2.7876e+01, -2.8177e+01, -3.1263e+01],\n",
      "        [-9.7909e+00, -2.9305e-02, -3.5514e+00, -1.0218e+01, -9.8534e+00,\n",
      "         -1.6113e+01, -9.9088e+00, -1.8881e+01],\n",
      "        [-8.9138e+00, -4.6433e-04, -8.9670e+00, -1.4603e+01, -1.2880e+01,\n",
      "         -1.2988e+01, -8.5329e+00, -1.7950e+01],\n",
      "        [-1.1228e+01, -5.5073e-05, -1.9197e+01, -2.0857e+01, -1.0084e+01,\n",
      "         -1.8773e+01, -1.7634e+01, -2.3469e+01],\n",
      "        [-1.7226e+01, -2.8010e-04, -1.8082e+01, -1.8253e+01, -8.1805e+00,\n",
      "         -1.7759e+01, -2.1361e+01, -2.4223e+01],\n",
      "        [-1.4345e+01, -6.5389e-04, -1.1798e+01, -1.1477e+01, -7.3702e+00,\n",
      "         -1.2178e+01, -1.4788e+01, -1.8827e+01],\n",
      "        [-5.6057e+00, -1.6396e-01, -3.0181e+00, -6.4427e+00, -2.3328e+00,\n",
      "         -1.4915e+01, -1.0342e+01, -1.8905e+01],\n",
      "        [-5.4870e+00, -1.1324e+01, -7.3734e+00, -1.1045e+01, -4.8152e-03,\n",
      "         -1.4428e+01, -1.1891e+01, -2.1606e+01],\n",
      "        [-1.7244e+01, -1.1921e-07, -1.6559e+01, -1.9878e+01, -1.6664e+01,\n",
      "         -1.7382e+01, -1.6643e+01, -2.2471e+01],\n",
      "        [-2.0673e+01,  0.0000e+00, -1.8261e+01, -2.3583e+01, -1.8425e+01,\n",
      "         -2.4783e+01, -2.5914e+01, -3.0767e+01],\n",
      "        [-1.8050e+01, -5.9605e-07, -1.7234e+01, -1.4574e+01, -1.6867e+01,\n",
      "         -1.5620e+01, -1.7242e+01, -1.8326e+01],\n",
      "        [-2.1681e+01,  0.0000e+00, -2.3551e+01, -1.9405e+01, -2.2007e+01,\n",
      "         -2.5855e+01, -2.7213e+01, -2.9788e+01],\n",
      "        [-1.2355e+01, -5.0068e-06, -1.5613e+01, -1.4985e+01, -1.5252e+01,\n",
      "         -2.5812e+01, -2.2969e+01, -2.8616e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "sentences_w_words \n",
      " [[39172, 9, 11, 825, 117, 22, 10, 1601, 4, 398, 98307, 11, 3122, 117, 11, 98307, 1931, 889, 23, 16, 407, 98307]]\n",
      "predicted_labels \n",
      " [3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_output \\n {model_output}\")\n",
    "# predicted_labels = torch.argmax(model_output, dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# predicted_labels = torch.argmax(torch.abs(model_output), dim=1)\n",
    "# print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "predicted_labels = np.argmax(model_output.detach().numpy(), axis=1)\n",
    "print(f\"sentences_w_words \\n {abc}\")\n",
    "print(f\"predicted_labels \\n {predicted_labels}\")\n",
    "# print(f\"correct_labels \\n {}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
